# Documentation: `run_train.py`

This script trains a deep learning model using TabPFN and a custom Transformer decoder to predict centroids for k-means clustering tasks. It is part of a research project exploring the use of Transformers to approximate NP-Hard problems.

---

## Overview

- **Goal:** Train a model to approximate k-means centroids on tabular datasets.
- **Libraries:** `torch`, `pandas`, `tabpfn`, `wandb`, `scipy.optimize`, and custom modules.
- **Input:** Tabular datasets and centroid labels generated by ELKI.
- **Output:** Trained model parameters saved to file.

---

## Main Components

### 1. `parse_arguments()`

Parses command-line arguments for:
- `--lr`: Learning rate.
- `--weight_decay`: Weight decay.
- `--patience`: Early stopping patience (unused currently).

### 2. `parse_centroids(file_path, device)`

Parses centroid text files into PyTorch tensors, each representing a k-means centroid.

### 3. Loss Functions

- `fixed_order_centroid_loss(predicted, target)`:  
  Sorts predicted/target centroids by distance to origin, computes MSE loss + penalty to prevent collapse.

- `hungarian_centroid_loss(predicted, target)`:  
  Uses Hungarian algorithm for permutation-invariant loss matching between predicted and target centroids.

### 4. `main(args)`

Core training logic:
- Initializes Weights & Biases (`wandb`) logging.
- Collects all training datasets and centroid files.
- Determines the maximum centroid dimension for padding purposes.
- For each dataset:
  - Loads dataset and corresponding centroid file.
  - Initializes:
    - **TabPFN model** (`_forward()` produces feature embeddings).
    - **Transformer decoder** (maps embeddings to predicted centroids).
  - Prepares padded tensors and auxiliary inputs.
  - Trains the model over 13 steps with backpropagation using selected loss.
  - Optionally includes commented-out validation loop with `wandb` logging.
- Saves model, decoder, and optimizer states to disk.

---

## Files and Directory Structure

- `new_data_3/train_no_header/*.csv`: Tabular training data.
- `new_data_3/train_result/*.csv`: Ground truth centroids.
- Output saved as: `./new_data_model_3.pth`

---

## Future Improvements

- Implement dynamic number of centroids.
- Enable validation and early stopping with patience.
- Support real-time wandb logging.
- Extend to multi-file training (batching).
- Optimize runtime and generalization across variable-dimension data.

---

## Usage

```bash
python run_train.py --lr 0.01 --weight_decay 0.01 --patience 3
```

---

## Authors

Part of the ERSP Research Project â€” UC San Diego  
Contributors: Nick Nguyen, Sophia Li, Myat Thiha, Kumiko Komori

