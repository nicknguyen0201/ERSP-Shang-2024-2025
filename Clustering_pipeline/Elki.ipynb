{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M78khc3b_JhJ",
        "outputId": "1afbe4df-0096-4833-84d5-f264d4f7cc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: JPype1 in /home/esrp2024/.local/lib/python3.10/site-packages (1.5.2)\n",
            "Requirement already satisfied: packaging in /home/esrp2024/.local/lib/python3.10/site-packages (from JPype1) (24.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install JPype1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import jpype\n",
        "from jpype.types import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import normalized_mutual_info_score, accuracy_score\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import sys\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "ELKI_JAR = \"/home/esrp2024/tmp/elki-bundle-0.8.0.jar\"\n",
        "#DATA_FOLDER = \"Clustering_pipeline/sub_sampling_cv(nick)\"  # Folder containing CSV files\n",
        "# 1. JVM Management\n",
        "if jpype.isJVMStarted():\n",
        "    print(\"JVM already running. Kernel restart recommended.\")\n",
        "else:\n",
        "    jpype.startJVM(\n",
        "        \"--add-opens=java.base/java.lang=ALL-UNNAMED\",\n",
        "        \"--add-opens=java.base/java.util=ALL-UNNAMED\",\n",
        "        classpath=[ELKI_JAR], \n",
        "        convertStrings=True\n",
        "    )\n",
        "\n",
        "# Load ELKI classes\n",
        "wcss = jpype.JClass('elki.clustering.kmeans.quality.WithinClusterVariance')\n",
        "LloydKMeans = jpype.JClass('elki.clustering.kmeans.LloydKMeans')\n",
        "StaticArrayDatabase = jpype.JClass('elki.database.StaticArrayDatabase')\n",
        "ArrayAdapterDatabaseConnection = jpype.JClass('elki.datasource.ArrayAdapterDatabaseConnection')\n",
        "EuclideanDistance = jpype.JClass('elki.distance.minkowski.EuclideanDistance')\n",
        "KMeansPlusPlus = jpype.JClass('elki.clustering.kmeans.initialization.KMeansPlusPlus')\n",
        "RandomFactory = jpype.JClass('elki.utilities.random.RandomFactory')\n",
        "NumberVector = jpype.JClass('elki.data.NumberVector')\n",
        "RandomlyChosen = jpype.JClass('elki.clustering.kmeans.initialization.RandomlyChosen')\n",
        "ElkanKMeans  = jpype.JClass('elki.clustering.kmeans.ElkanKMeans')\n",
        "YinYangKMeans = jpype.JClass('elki.clustering.kmeans.YinYangKMeans')\n",
        "AnnulusKMeans = jpype.JClass('elki.clustering.kmeans.AnnulusKMeans')\n",
        "HamerlyKMeans = jpype.JClass('elki.clustering.kmeans.HamerlyKMeans')\n",
        "ShallotKMeans = jpype.JClass('elki.clustering.kmeans.ShallotKMeans')\n",
        "ExponionKMeans = jpype.JClass('elki.clustering.kmeans.ExponionKMeans')\n",
        "BestOfMultipleKMeans = jpype.JClass('elki.clustering.kmeans.BestOfMultipleKMeans')\n",
        "BetulaLloydKMeans = jpype.JClass('elki.clustering.kmeans.BetulaLloydKMeans')\n",
        "BisectingKMeans = jpype.JClass('elki.clustering.kmeans.BisectingKMeans')\n",
        "CompareMeans = jpype.JClass('elki.clustering.kmeans.CompareMeans')\n",
        "FuzzyCMeans = jpype.JClass('elki.clustering.kmeans.FuzzyCMeans')\n",
        "GMeans = jpype.JClass('elki.clustering.kmeans.GMeans')\n",
        "HartiganWongKMeans = jpype.JClass('elki.clustering.kmeans.HartiganWongKMeans')\n",
        "KDTreeFilteringKMeans = jpype.JClass('elki.clustering.kmeans.KDTreeFilteringKMeans')\n",
        "KDTreePruningKMeans = jpype.JClass('elki.clustering.kmeans.KDTreePruningKMeans')\n",
        "KMeansMinusMinus = jpype.JClass('elki.clustering.kmeans.KMeansMinusMinus')\n",
        "KMediansLloyd = jpype.JClass('elki.clustering.kmeans.KMediansLloyd')\n",
        "MacQueenKMeans = jpype.JClass('elki.clustering.kmeans.MacQueenKMeans')\n",
        "SimplifiedElkanKMeans = jpype.JClass('elki.clustering.kmeans.SimplifiedElkanKMeans')\n",
        "SingleAssignmentKMeans = jpype.JClass('elki.clustering.kmeans.SingleAssignmentKMeans')\n",
        "XMeans = jpype.JClass('elki.clustering.kmeans.XMeans')\n",
        "SortMeans = jpype.JClass('elki.clustering.kmeans.SortMeans')\n",
        "ParallelLloydKMeans = jpype.JClass('elki.clustering.kmeans.parallel.ParallelLloydKMeans')\n",
        "KMeansQualityMeasure = jpype.JClass('elki.clustering.kmeans.quality.KMeansQualityMeasure')\n",
        "SimplifiedElkanKMeans = jpype.JClass('elki.clustering.kmeans.SimplifiedElkanKMeans')\n",
        "WithinClusterVariance = JClass(\"elki.clustering.kmeans.quality.WithinClusterVariance\")\n",
        "KDTreePruningKMeansSplit = JClass(\"elki.clustering.kmeans.KDTreePruningKMeans$Split\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_file(file):\n",
        "    print(file)\n",
        "    df = pd.read_csv(file)  # Use the file variable from the loop\n",
        "    # Drop the first row (which might contain labels or bad data)\n",
        "    df = df.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    # Assume the last column is the target/label column\n",
        "    true_labels = df.iloc[:, -1].values  # Extract labels from last column\n",
        "    df = df.iloc[:, :-1]                 # Drop last column (features only remain)\n",
        "\n",
        "    # Count number of unique labels\n",
        "    k_centroids = len(np.unique(true_labels))\n",
        "    print(f\"k = {k_centroids}\")\n",
        "    \"\"\"\n",
        "    # Extract ground truth labels if available\n",
        "    if 'target' in df.columns:\n",
        "        true_labels = df['target'].values  # Ground truth labels for evaluation\n",
        "        df = df.drop(columns=['target'])   # Drop label column before clustering\n",
        "        df = df.drop(index=0).reset_index(drop=True)\n",
        "        k_centroids =len(np.unique(true_labels))\n",
        "        print(f\"k=\",k_centroids)\n",
        "    else:\n",
        "        true_labels = None\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure only numeric columns are used\n",
        "    data_values = df.select_dtypes(include=['number']).values.astype(float)\n",
        "\n",
        "    # Convert to 2D Java array (double[][])\n",
        "    java_data = jpype.JArray(JDouble, 2)(data_values)\n",
        "\n",
        "    # Prepare ELKI database\n",
        "    adapter = ArrayAdapterDatabaseConnection(java_data)\n",
        "    database = StaticArrayDatabase(adapter, [])\n",
        "    database.initialize()\n",
        "\n",
        "    # Find the correct relation of type NumberVector\n",
        "    relation = None\n",
        "    for rel in database.getRelations():\n",
        "        if NumberVector.class_.isAssignableFrom(rel.getDataTypeInformation().getRestrictionClass()):\n",
        "            relation = rel\n",
        "            break\n",
        "\n",
        "    if relation is None:\n",
        "        raise ValueError(\"No valid Relation<NumberVector> found in the database!\")\n",
        "\n",
        "    # Debugging: Print the number of relations in the database\n",
        "    #print(\"Total Relations in Database:\", len(database.getRelations()))\n",
        "\n",
        "    # Debugging: Check if relation exists and print details\n",
        "    if relation:\n",
        "        #print(\"✅ Relation successfully found!\")\n",
        "        #print(\"Total Data Points in Relation:\", relation.size())\n",
        "\n",
        "        # Fetch a few data points from relation (convert from ELKI to Python)\n",
        "        iter_dbid = relation.iterDBIDs()  # Get all IDs\n",
        "        sample_points = []\n",
        "        for _ in range(5):  # Print first 5 points\n",
        "            if not iter_dbid.valid():\n",
        "                break  # Stop if there are no more data points\n",
        "            obj = relation.get(iter_dbid)\n",
        "            sample_points.append([obj.doubleValue(i) for i in range(obj.getDimensionality())])\n",
        "            iter_dbid.advance()\n",
        "\n",
        "    return relation, data_values, true_labels, k_centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_centroids_pos(result):\n",
        "    centroids = []\n",
        "    for cluster in result.getAllClusters():\n",
        "        if not cluster.isNoise():\n",
        "            model = cluster.getModel()  # ✅ Extract KMeansModel\n",
        "            if model is not None:\n",
        "                centroid = centroid = list(model.getMean()) \n",
        "                centroids.append(centroid)  # ✅ Store centroid\n",
        "    centroids = np.array(centroids)\n",
        "    return centroids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clustering(relation,data_values,kmeans):     \n",
        "   \n",
        "    \n",
        "    \n",
        "    # Start the timer (tic)\n",
        "    start_time = time.perf_counter()\n",
        "    result = kmeans.run(relation)\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    clustering_time = end_time - start_time\n",
        "\n",
        "    # Extract cluster assignments\n",
        "    cluster_labels = np.full(len(data_values), -1)  # Initialize with -1 (unassigned)\n",
        "    # Build the cluster labels array based on the relation's DBIDs.\n",
        "    dbid_range = relation.getDBIDs()        # This is an IntegerDBIDRange for all data points.\n",
        "    num_dbids = dbid_range.size()             # Should be 150 for the iris dataset.\n",
        "    cluster_labels = np.full(num_dbids, -1)   # Initialize with -1 (unassigned)\n",
        "\n",
        "    for cluster_id, cluster in enumerate(result.getAllClusters()):\n",
        "        #print(\"Processing cluster_ID:\", cluster_id)\n",
        "        if not cluster.isNoise():\n",
        "            # Loop over all DBIDs (in the same order as the data_values)\n",
        "            for i in range(num_dbids):\n",
        "                dbid = dbid_range.get(i)  # Get the DBID corresponding to the i-th data point.\n",
        "                # Check if this DBID is in the cluster's set.\n",
        "                if cluster.getIDs().contains(dbid):\n",
        "                    cluster_labels[i] = cluster_id\n",
        "                    #print(f\"Assigned cluster {cluster_id} to index {i}\")\n",
        "\n",
        "    centroids_pos = get_centroids_pos(result)\n",
        "    \n",
        "    return result, cluster_labels,centroids_pos, clustering_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_inertia(relation, result):\n",
        "    SquaredErrors = jpype.JClass('elki.evaluation.clustering.internal.SquaredErrors')\n",
        "    NoiseHandling = jpype.JClass('elki.evaluation.clustering.internal.NoiseHandling')\n",
        "    available_constants = [const.name() for const in NoiseHandling.values()]\n",
        "    #print(\"Available NoiseHandling constants:\", available_constants)\n",
        "\n",
        "    # Suppose you have:\n",
        "    #   database:    a StaticArrayDatabase\n",
        "    #   relation:    a Relation<NumberVector>\n",
        "    #   clustering:  the result of LloydKMeans.run(relation)\n",
        "\n",
        "    # 1. Choose a distance and noise handling\n",
        "    distance = EuclideanDistance()\n",
        "    # List all enum values if you're unsure which is available:\n",
        "    #   print([v.name() for v in NoiseHandling.values()])\n",
        "    noise_handling = NoiseHandling.valueOf(\"MERGE_NOISE\")  # or another valid enum\n",
        "\n",
        "    # 2. Create the evaluator\n",
        "    sse_evaluator = SquaredErrors(distance, noise_handling)\n",
        "\n",
        "    # 3. Compute SSE (sum of squared errors)\n",
        "    sse = sse_evaluator.evaluateClustering(relation, result)\n",
        "    return sse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_accuracy(true_labels, predicted_labels):\n",
        "    if true_labels is None:\n",
        "         return None\n",
        "    # Create a contingency (confusion) matrix\n",
        "    C = confusion_matrix(true_labels, predicted_labels)\n",
        "    # print(C)\n",
        "    # Use the Hungarian algorithm to maximize the total correct assignments\n",
        "    #row_ind and col_ind are 2 numpy array\n",
        "    row_ind, col_ind = linear_sum_assignment(-C)  # We use negative because we want to maximize\n",
        "    #print(\"row_ind \", row_ind)\n",
        "    #print(\"col_ind \",col_ind)\n",
        "    # Sum the counts from the optimal assignment\n",
        "    total_correct = C[row_ind, col_ind].sum()\n",
        "    #print(\"total_correct \", total_correct)\n",
        "    # Calculate accuracy as the ratio of correctly assigned samples\n",
        "    accuracy = total_correct / np.sum(C)\n",
        "    #print (\"sum(C) \", np.sum(C))\n",
        "    #print(accuracy)\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_evaluation_scores(true_labels, predicted_labels):\n",
        "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels) if true_labels is not None else None\n",
        "    accuracy_score= compute_accuracy(true_labels, predicted_labels)\n",
        "    print(f\"NMI: \",nmi_score)\n",
        "    print(f\"accuracy_score: \",accuracy_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_best_algorithm_and_write_centroids(algorithms_map, output_file=\"centroids.txt\"):\n",
        "    \"\"\"\n",
        "    Evaluates a dictionary of algorithms (each mapped to a tuple of \n",
        "    (time, accuracy, centroids)) to find the best algorithm based on \n",
        "    the lowest time/accuracy ratio. After picking the best algorithm, \n",
        "    this function writes the positions of its k centroids into a text file.\n",
        "\n",
        "    Parameters:\n",
        "        algorithms_map (dict): Dictionary where each key is an algorithm name and \n",
        "                               each value is a tuple (time, accuracy, centroids).\n",
        "                               For example:\n",
        "                               {\n",
        "                                   \"Algorithm A\": (120, 0.85, [(x1, y1), (x2, y2), ...]),\n",
        "                                   \"Algorithm B\": (100, 0.80, [(x1, y1), (x2, y2), ...]),\n",
        "                                   ...\n",
        "                               }\n",
        "        output_file (str): The file path to write the centroids' positions (default \"centroids.txt\").\n",
        "\n",
        "    Returns:\n",
        "        tuple: (best_algo_name, best_ratio) where best_algo_name is the name of the best algorithm,\n",
        "               and best_ratio is its time/accuracy ratio.\n",
        "    \"\"\"\n",
        "    best_algo_name = None\n",
        "    best_ratio = 0\n",
        "    \n",
        "    # Iterate over each algorithm to compute the time/accuracy ratio.\n",
        "    for algo_name, metrics in algorithms_map.items():\n",
        "        # Since metrics is a tuple: (time, accuracy, centroids)\n",
        "        time_val, accuracy_val, centroids = metrics\n",
        "        \n",
        "        # Skip algorithms with missing metrics or accuracy of zero.\n",
        "        if time_val is None or accuracy_val is None or accuracy_val == 0:\n",
        "            continue\n",
        "        \n",
        "        ratio = accuracy_val\n",
        "        print(f\"Algorithm: {algo_name}, Time: {time_val}, Accuracy: {accuracy_val}, Ratio: {ratio:.3f}\")\n",
        "        \"\"\" what is this find max :)))))\n",
        "        if ratio < best_ratio:\n",
        "            best_ratio = ratio\n",
        "            best_algo_name = algo_name\n",
        "        \"\"\"\n",
        "        if ratio > best_ratio:\n",
        "            best_ratio=ratio \n",
        "            best_algo_name = algo_name\n",
        "            \n",
        "    if best_algo_name is None:\n",
        "        print(\"No valid algorithm found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Retrieve centroids from the best algorithm's tuple.\n",
        "    _, _, centroids_best = algorithms_map[best_algo_name]\n",
        "    \n",
        "    if centroids_best is None:\n",
        "        print(\"Best algorithm does not contain centroid information.\")\n",
        "        return best_algo_name, best_ratio\n",
        "\n",
        "    # Write the centroid positions to the specified text file.\n",
        "    with open(output_file, \"w\") as file:\n",
        "        #file.write(f\"Centroids for {best_algo_name} (Ratio: {best_ratio:.3f}):\\n\")\n",
        "        for idx, centroid in enumerate(centroids_best, start=1):\n",
        "            file.write(f\"{centroid}\\n\")\n",
        "    \n",
        "    print(f\"Centroid positions written to {output_file}\")\n",
        "    print(f\"\\nThe best algorithm is {best_algo_name} with a time/accuracy ratio of {best_ratio:.3f}.\")\n",
        "    return best_algo_name, best_ratio\n",
        "\n",
        "\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_iter=300 \n",
        "algorithm_constructors = [\n",
        "        lambda k_centroids: LloydKMeans(EuclideanDistance(), k_centroids, max_iter, RandomlyChosen(RandomFactory(42))),\n",
        "        lambda k_centroids: LloydKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: ElkanKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: SimplifiedElkanKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: YinYangKMeans(k_centroids, max_iter, RandomlyChosen(RandomFactory(42)), 5),\n",
        "        lambda k_centroids: AnnulusKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: HamerlyKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: ShallotKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: ExponionKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: BestOfMultipleKMeans(10,LloydKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),wcss()),\n",
        "        lambda k_centroids: CompareMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: FuzzyCMeans(k_centroids, 10, max_iter, 0.0001, 2,False, KMeansPlusPlus(RandomFactory(42)) ),             \n",
        "        lambda k_centroids: HartiganWongKMeans( k_centroids,KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: KDTreeFilteringKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), KDTreePruningKMeans.Split.SSQ,40),\n",
        "        lambda k_centroids: KMeansMinusMinus(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)),0.05, True),\n",
        "        lambda k_centroids: KDTreePruningKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), KDTreePruningKMeansSplit.SSQ,40),\n",
        "        lambda k_centroids: KMediansLloyd(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: MacQueenKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: SimplifiedElkanKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: SingleAssignmentKMeans(EuclideanDistance(), k_centroids,KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: SortMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: ParallelLloydKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)))\n",
        "]\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "lambda k_centroids: GMeans(EuclideanDistance(),0.05, k_centroids, k_centroids,max_iter,\n",
        "                                AnnulusKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True), \n",
        "                                KMeansPlusPlus(RandomFactory(42)), RandomFactory(42)),\n",
        " lambda k_centroids: XMeans(\n",
        "        EuclideanDistance(),  # Distance function\n",
        "        k_centroids,  # Minimum number of clusters\n",
        "        k_centroids * 2,  # Maximum number of clusters (XMeans can return up to 2*k)\n",
        "        max_iter,  # Maximum iterations\n",
        "        LloydKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),  # Inner K-Means variant\n",
        "        KMeansPlusPlus(RandomFactory(42)),  # Initialization method\n",
        "        WithinClusterVariance(),  # Correct quality measure\n",
        "        RandomFactory(42)  # Random factory\n",
        "        ),\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cv/train_cv_w_header/train_cv_dataset_46591.csv\n",
            "k = 2\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@74f6c5d8, Time: 0.000747166108340025, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@19b89d4, Time: 0.0006053256802260876, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.ElkanKMeans@2415fc55, Time: 0.0013804808259010315, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@93081b6, Time: 0.0011626733466982841, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.YinYangKMeans@15a04efb, Time: 0.0007271845825016499, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.AnnulusKMeans@26adfd2d, Time: 0.000694845337420702, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.HamerlyKMeans@6950ed69, Time: 0.0006415559910237789, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.ShallotKMeans@48b67364, Time: 0.001104053109884262, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.ExponionKMeans@39b43d60, Time: 0.0009553739801049232, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.BestOfMultipleKMeans@4fa1c212, Time: 0.0026874830946326256, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.CompareMeans@4e928fbf, Time: 0.000313648022711277, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.FuzzyCMeans@748741cb, Time: 0.0019209468737244606, Accuracy: 0.9516129032258065, Ratio: 0.952\n",
            "Algorithm: elki.clustering.kmeans.HartiganWongKMeans@5f77d0f9, Time: 0.0012717321515083313, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.KDTreeFilteringKMeans@7a3793c7, Time: 0.007904878351837397, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.KMeansMinusMinus@5456afaa, Time: 0.0004281667061150074, Accuracy: 0.9050179211469535, Ratio: 0.905\n",
            "Algorithm: elki.clustering.kmeans.KDTreePruningKMeans@39d76cb5, Time: 0.0078782276250422, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.KMediansLloyd@50313382, Time: 0.00795070780441165, Accuracy: 0.9516129032258065, Ratio: 0.952\n",
            "Algorithm: elki.clustering.kmeans.MacQueenKMeans@4d518b32, Time: 0.000368588138371706, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@7354b8c5, Time: 0.0006667561829090118, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.SingleAssignmentKMeans@479cbee5, Time: 7.904879748821259e-05, Accuracy: 0.6254480286738351, Ratio: 0.625\n",
            "Algorithm: elki.clustering.kmeans.SortMeans@5d43661b, Time: 0.00030801771208643913, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Algorithm: elki.clustering.kmeans.parallel.ParallelLloydKMeans@1ae8bcbc, Time: 0.03406731691211462, Accuracy: 0.9551971326164874, Ratio: 0.955\n",
            "Centroid positions written to cv/train_cv_result/centroids_train_cv_dataset_46591.csv\n",
            "\n",
            "The best algorithm is elki.clustering.kmeans.LloydKMeans@74f6c5d8 with a time/accuracy ratio of 0.955.\n",
            "Best centroids saved to cv/train_cv_result/centroids_train_cv_dataset_46591.csv\n",
            "cv/train_cv_w_header/train_cv_dataset_46542.csv\n",
            "k = 2\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@1f75a668, Time: 0.0022072349674999714, Accuracy: 0.5081351689612015, Ratio: 0.508\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@5170bcf4, Time: 0.001003422774374485, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.ElkanKMeans@1a72a540, Time: 0.0016615795902907848, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@5117dd67, Time: 0.0011830921284854412, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.YinYangKMeans@19e7a160, Time: 0.001910508144646883, Accuracy: 0.5319148936170213, Ratio: 0.532\n",
            "Algorithm: elki.clustering.kmeans.AnnulusKMeans@327af41b, Time: 0.0015451298095285892, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.HamerlyKMeans@6aa61224, Time: 0.0016501592472195625, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.ShallotKMeans@2474f125, Time: 0.0010996432974934578, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.ExponionKMeans@68f4865, Time: 0.0010214229114353657, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.BestOfMultipleKMeans@18e36d14, Time: 0.01188587211072445, Accuracy: 0.5056320400500626, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.CompareMeans@332729ad, Time: 0.0009852442890405655, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.FuzzyCMeans@3370f42, Time: 0.0025498527102172375, Accuracy: 0.5269086357947435, Ratio: 0.527\n",
            "Algorithm: elki.clustering.kmeans.HartiganWongKMeans@6853425f, Time: 0.0029714698903262615, Accuracy: 0.5018773466833542, Ratio: 0.502\n",
            "Algorithm: elki.clustering.kmeans.KDTreeFilteringKMeans@282cb7c7, Time: 0.011983920820057392, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.KMeansMinusMinus@14f9390f, Time: 0.0011745919473469257, Accuracy: 0.48685857321652065, Ratio: 0.487\n",
            "Algorithm: elki.clustering.kmeans.KDTreePruningKMeans@409c54f, Time: 0.012155420146882534, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.KMediansLloyd@71e693fa, Time: 0.019479312002658844, Accuracy: 0.5181476846057572, Ratio: 0.518\n",
            "Algorithm: elki.clustering.kmeans.MacQueenKMeans@7b8b43c7, Time: 0.10027579264715314, Accuracy: 0.5081351689612015, Ratio: 0.508\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@60b4beb4, Time: 0.0028018620796501637, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.SingleAssignmentKMeans@41294f8, Time: 0.0001913490705192089, Accuracy: 0.5994993742177722, Ratio: 0.599\n",
            "Algorithm: elki.clustering.kmeans.SortMeans@479ceda0, Time: 0.0017692488618195057, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.parallel.ParallelLloydKMeans@38b27cdc, Time: 0.057751590851694345, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Centroid positions written to cv/train_cv_result/centroids_train_cv_dataset_46542.csv\n",
            "\n",
            "The best algorithm is elki.clustering.kmeans.SingleAssignmentKMeans@41294f8 with a time/accuracy ratio of 0.599.\n",
            "Best centroids saved to cv/train_cv_result/centroids_train_cv_dataset_46542.csv\n",
            "cv/train_cv_w_header/train_cv_dataset_46540.csv\n",
            "k = 2\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@68df9280, Time: 0.0021906462498009205, Accuracy: 0.5081351689612015, Ratio: 0.508\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@5be46f9d, Time: 0.0009896541014313698, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.ElkanKMeans@2421cc4, Time: 0.001819457858800888, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@21ba0741, Time: 0.0011794217862188816, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.YinYangKMeans@2796aeae, Time: 0.0019185468554496765, Accuracy: 0.5319148936170213, Ratio: 0.532\n",
            "Algorithm: elki.clustering.kmeans.AnnulusKMeans@62010f5c, Time: 0.0014975513331592083, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.HamerlyKMeans@5b6ec132, Time: 0.0015591387636959553, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.ShallotKMeans@132e0cc, Time: 0.0010742326267063618, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.ExponionKMeans@6cc558c6, Time: 0.0009864331223070621, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.BestOfMultipleKMeans@291a7e3c, Time: 0.011684042867273092, Accuracy: 0.5056320400500626, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.CompareMeans@416c58f5, Time: 0.0010061739012598991, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.FuzzyCMeans@2bb3058, Time: 0.0025731627829372883, Accuracy: 0.5269086357947435, Ratio: 0.527\n",
            "Algorithm: elki.clustering.kmeans.HartiganWongKMeans@14fc1f0, Time: 0.002989590633660555, Accuracy: 0.5018773466833542, Ratio: 0.502\n",
            "Algorithm: elki.clustering.kmeans.KDTreeFilteringKMeans@1c93f6e1, Time: 0.011951391585171223, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.KMeansMinusMinus@e383572, Time: 0.001137763261795044, Accuracy: 0.48685857321652065, Ratio: 0.487\n",
            "Algorithm: elki.clustering.kmeans.KDTreePruningKMeans@2fd953a6, Time: 0.012487548403441906, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.KMediansLloyd@3abada5a, Time: 0.019580780994147062, Accuracy: 0.5181476846057572, Ratio: 0.518\n",
            "Algorithm: elki.clustering.kmeans.MacQueenKMeans@6ccdb29f, Time: 0.0006741462275385857, Accuracy: 0.5081351689612015, Ratio: 0.508\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@7a1a14a4, Time: 0.0012312917970120907, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.SingleAssignmentKMeans@4b41dd5c, Time: 9.79788601398468e-05, Accuracy: 0.5994993742177722, Ratio: 0.599\n",
            "Algorithm: elki.clustering.kmeans.SortMeans@5ba88be8, Time: 0.0010130838491022587, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.parallel.ParallelLloydKMeans@3a4621bd, Time: 0.04941300582140684, Accuracy: 0.5031289111389237, Ratio: 0.503\n",
            "Centroid positions written to cv/train_cv_result/centroids_train_cv_dataset_46540.csv\n",
            "\n",
            "The best algorithm is elki.clustering.kmeans.SingleAssignmentKMeans@4b41dd5c with a time/accuracy ratio of 0.599.\n",
            "Best centroids saved to cv/train_cv_result/centroids_train_cv_dataset_46540.csv\n",
            "cv/train_cv_w_header/train_cv_dataset_46876.csv\n",
            "k = 2\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@40db2a24, Time: 0.0010028029792010784, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@177bea38, Time: 0.0009790239855647087, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.ElkanKMeans@6ed3f258, Time: 0.0020703556947410107, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@68b32e3e, Time: 0.0014972700737416744, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.YinYangKMeans@33308786, Time: 0.001610409002751112, Accuracy: 0.6961471103327496, Ratio: 0.696\n",
            "Algorithm: elki.clustering.kmeans.AnnulusKMeans@63787180, Time: 0.0020301067270338535, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.HamerlyKMeans@22356acd, Time: 0.0016308799386024475, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.ShallotKMeans@3d1848cc, Time: 0.0013822796754539013, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.ExponionKMeans@344f4dea, Time: 0.0012158318422734737, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.BestOfMultipleKMeans@6436a7db, Time: 0.00876375287771225, Accuracy: 0.6751313485113836, Ratio: 0.675\n",
            "Algorithm: elki.clustering.kmeans.CompareMeans@25aca718, Time: 0.0009900243021547794, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.FuzzyCMeans@7776ab, Time: 0.003195208963006735, Accuracy: 0.5306479859894921, Ratio: 0.531\n",
            "Algorithm: elki.clustering.kmeans.HartiganWongKMeans@6a55299e, Time: 0.003862443845719099, Accuracy: 0.7084063047285464, Ratio: 0.708\n",
            "Algorithm: elki.clustering.kmeans.KDTreeFilteringKMeans@3a0baae5, Time: 0.0291758980602026, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.KMeansMinusMinus@403f0a22, Time: 0.0010950523428618908, Accuracy: 0.6865148861646234, Ratio: 0.687\n",
            "Algorithm: elki.clustering.kmeans.KDTreePruningKMeans@5143c662, Time: 0.029498655814677477, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.KMediansLloyd@3e7dd664, Time: 0.051147114019840956, Accuracy: 0.690893169877408, Ratio: 0.691\n",
            "Algorithm: elki.clustering.kmeans.MacQueenKMeans@1b835480, Time: 0.00029436778277158737, Accuracy: 0.7644483362521891, Ratio: 0.764\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@6e950bcf, Time: 0.0013615009374916553, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.SingleAssignmentKMeans@46dffdc3, Time: 0.00011397898197174072, Accuracy: 0.637478108581436, Ratio: 0.637\n",
            "Algorithm: elki.clustering.kmeans.SortMeans@53dbe163, Time: 0.0009883642196655273, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Algorithm: elki.clustering.kmeans.parallel.ParallelLloydKMeans@7a1a3478, Time: 0.0352608491666615, Accuracy: 0.7066549912434326, Ratio: 0.707\n",
            "Centroid positions written to cv/train_cv_result/centroids_train_cv_dataset_46876.csv\n",
            "\n",
            "The best algorithm is elki.clustering.kmeans.MacQueenKMeans@1b835480 with a time/accuracy ratio of 0.764.\n",
            "Best centroids saved to cv/train_cv_result/centroids_train_cv_dataset_46876.csv\n"
          ]
        }
      ],
      "source": [
        "def main(input_dir, output_dir=\"results\"):\n",
        "    # Create results directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "\n",
        "    # Automatically find all CSV files in the provided directory.\n",
        "    csv_files = glob.glob(os.path.join(input_dir, '*.csv'))\n",
        "    if not csv_files:\n",
        "        print(\"No CSV files found in the specified directory.\")\n",
        "        return\n",
        "    \n",
        "    # Process each CSV file individually\n",
        "\n",
        "    for file in csv_files:\n",
        "        #process the file \n",
        "\n",
        "        #uncomment to mass process\n",
        "        relation,data_values,true_labels,k_centroids = process_file(file)\n",
        "\n",
        "        # uncomment to run any single dataset\n",
        "        #relation,data_values,true_labels,k_centroids = process_file(\"/home/esrp2024/tmp/train/train_dataset_1485.csv\")\n",
        "        \n",
        "        #list of algors\n",
        "        #algorithms_list=[\"LloydKMeans\",\"KMeansPlusPlus\",\"ElkanKmeans\",\"YinYangKMeans\",\"AnnulusKMeans\",\"HamerlyKMeans\"]\n",
        "        #empty map\n",
        "        algorithms_map = {}\n",
        "\n",
        "        #for algorithm_name in algorithms_list:\n",
        "        for constructor in algorithm_constructors:\n",
        "            kmeans = constructor(k_centroids)\n",
        "            # Get the base name of the file (i.e., the file name without the path)\n",
        "            #basename = os.path.basename(file)\n",
        "            \n",
        "            \n",
        "            \n",
        "            result,predicted_labels,centroids_pos,clustering_time = clustering(relation, data_values,kmeans)\n",
        "            #inertia=calculate_inertia(relation, result)\n",
        "            acc=compute_accuracy(true_labels, predicted_labels)\n",
        "            \n",
        "            #map\n",
        "            algorithms_map[kmeans] = (clustering_time, acc, centroids_pos)\n",
        "            \n",
        "         # Generate a unique output file for each dataset inside the \"results\" folder\n",
        "        dataset_name = os.path.basename(file).replace('.csv', '')  # Extract dataset name\n",
        "        centroids_filename = os.path.join(output_dir, f\"centroids_{dataset_name}.csv\")  # Save best centroids\n",
        "\n",
        "        # Choose the best algorithm and write only the best centroids to a dataset-specific file\n",
        "        choose_best_algorithm_and_write_centroids(algorithms_map, output_file=centroids_filename)\n",
        "\n",
        "        print(f\"Best centroids saved to {centroids_filename}\")\n",
        "\n",
        "        \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='Process multiple CSV datasets from a directory.')\n",
        "    parser.add_argument('--input_dir', type=str, default='cv/train_cv_w_header', \n",
        "                        help='Directory containing CSV files (default is \"sub_sampling_cv(nick)\")')\n",
        "    parser.add_argument('--output_dir', type=str, default='cv/train_cv_result', \n",
        "                        help='Directory to save the results (default is \"results\")')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args.input_dir, args.output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: train_cv_dataset_46591.csv\n",
            "Processed: train_cv_dataset_46542.csv\n",
            "Processed: train_cv_dataset_46540.csv\n",
            "Processed: train_cv_dataset_46876.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Input and output folders\n",
        "input_folder = \"cv/train_cv_w_header\"  # Folder containing CSV files\n",
        "output_folder = \"cv/train_cv_no_header\"\n",
        "#output_folder2 = \"test_w_exact_true_labels\"\n",
        "# Create output folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Process all .csv files in the input folder\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "        true_labels_path = os.path.join(output_folder2, \"exact_true_labels_\" + filename)\n",
        "        # Read without assuming header\n",
        "        df = pd.read_csv(input_path, header=None)\n",
        "\n",
        "        # Drop first row and last column\n",
        "        df = df.drop(index=0).reset_index(drop=True)\n",
        "        true_labels = df.iloc[:, -1].values  # Extract labels from last column\n",
        "        df = df.iloc[:, :-1]\n",
        "\n",
        "        # Save to output folder with the same filename\n",
        "        df.to_csv(output_path, index=False, header=False)\n",
        "        #pd.Series(true_labels).to_csv(true_labels_path, index=False, header=False)\n",
        "        print(f\"Processed: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV files found: 45\n"
          ]
        }
      ],
      "source": [
        "def count_csv_files(folder_path):\n",
        "    return sum(1 for file in os.listdir(folder_path) if file.endswith('.csv'))\n",
        "folder = \"/home/esrp2024/tmp/results_for_test_data\"\n",
        "print(f\"CSV files found: {count_csv_files(folder)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (base)",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
