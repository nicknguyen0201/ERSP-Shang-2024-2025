{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M78khc3b_JhJ",
        "outputId": "1afbe4df-0096-4833-84d5-f264d4f7cc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: JPype1 in /home/esrp2024/.local/lib/python3.10/site-packages (1.5.2)\n",
            "Requirement already satisfied: packaging in /home/esrp2024/.local/lib/python3.10/site-packages (from JPype1) (24.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install JPype1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/home/esrp2024/anaconda3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/home/esrp2024/anaconda3/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/home/esrp2024/anaconda3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/home/esrp2024/anaconda3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/home/esrp2024/anaconda3/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipykernel_1348078/2116532632.py\", line 3, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/pandas/__init__.py\", line 26, in <module>\n",
            "    from pandas.compat import (\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
            "    from pandas.compat.pyarrow import (\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
            "    import pyarrow as pa\n",
            "  File \"/home/esrp2024/anaconda3/lib/python3.10/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.5 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"/home/esrp2024/anaconda3/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/home/esrp2024/anaconda3/lib/python3.10/runpy.py\", line 86, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/home/esrp2024/anaconda3/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/home/esrp2024/anaconda3/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
            "    handle._run()\n",
            "  File \"/home/esrp2024/anaconda3/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
            "    await result\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
            "    await super().execute_request(stream, ident, parent)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipykernel_1348078/2116532632.py\", line 3, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
            "    from pandas.core.api import (\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/pandas/core/api.py\", line 9, in <module>\n",
            "    from pandas.core.dtypes.dtypes import (\n",
            "  File \"/home/esrp2024/.local/lib/python3.10/site-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
            "    from pandas._libs import (\n",
            "  File \"/home/esrp2024/anaconda3/lib/python3.10/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        }
      ],
      "source": [
        "import jpype\n",
        "from jpype.types import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import normalized_mutual_info_score, accuracy_score\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import sys\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "ELKI_JAR = \"/home/esrp2024/tmp/elki-bundle-0.8.0.jar\"\n",
        "#DATA_FOLDER = \"Clustering_pipeline/sub_sampling_cv(nick)\"  # Folder containing CSV files\n",
        "# 1. JVM Management\n",
        "if jpype.isJVMStarted():\n",
        "    print(\"JVM already running. Kernel restart recommended.\")\n",
        "else:\n",
        "    jpype.startJVM(\n",
        "        \"--add-opens=java.base/java.lang=ALL-UNNAMED\",\n",
        "        \"--add-opens=java.base/java.util=ALL-UNNAMED\",\n",
        "        classpath=[ELKI_JAR], \n",
        "        convertStrings=True\n",
        "    )\n",
        "\n",
        "# Load ELKI classes\n",
        "wcss = jpype.JClass('elki.clustering.kmeans.quality.WithinClusterVariance')\n",
        "LloydKMeans = jpype.JClass('elki.clustering.kmeans.LloydKMeans')\n",
        "StaticArrayDatabase = jpype.JClass('elki.database.StaticArrayDatabase')\n",
        "ArrayAdapterDatabaseConnection = jpype.JClass('elki.datasource.ArrayAdapterDatabaseConnection')\n",
        "EuclideanDistance = jpype.JClass('elki.distance.minkowski.EuclideanDistance')\n",
        "KMeansPlusPlus = jpype.JClass('elki.clustering.kmeans.initialization.KMeansPlusPlus')\n",
        "RandomFactory = jpype.JClass('elki.utilities.random.RandomFactory')\n",
        "NumberVector = jpype.JClass('elki.data.NumberVector')\n",
        "RandomlyChosen = jpype.JClass('elki.clustering.kmeans.initialization.RandomlyChosen')\n",
        "ElkanKMeans  = jpype.JClass('elki.clustering.kmeans.ElkanKMeans')\n",
        "YinYangKMeans = jpype.JClass('elki.clustering.kmeans.YinYangKMeans')\n",
        "AnnulusKMeans = jpype.JClass('elki.clustering.kmeans.AnnulusKMeans')\n",
        "HamerlyKMeans = jpype.JClass('elki.clustering.kmeans.HamerlyKMeans')\n",
        "ShallotKMeans = jpype.JClass('elki.clustering.kmeans.ShallotKMeans')\n",
        "ExponionKMeans = jpype.JClass('elki.clustering.kmeans.ExponionKMeans')\n",
        "BestOfMultipleKMeans = jpype.JClass('elki.clustering.kmeans.BestOfMultipleKMeans')\n",
        "BetulaLloydKMeans = jpype.JClass('elki.clustering.kmeans.BetulaLloydKMeans')\n",
        "BisectingKMeans = jpype.JClass('elki.clustering.kmeans.BisectingKMeans')\n",
        "CompareMeans = jpype.JClass('elki.clustering.kmeans.CompareMeans')\n",
        "FuzzyCMeans = jpype.JClass('elki.clustering.kmeans.FuzzyCMeans')\n",
        "GMeans = jpype.JClass('elki.clustering.kmeans.GMeans')\n",
        "HartiganWongKMeans = jpype.JClass('elki.clustering.kmeans.HartiganWongKMeans')\n",
        "KDTreeFilteringKMeans = jpype.JClass('elki.clustering.kmeans.KDTreeFilteringKMeans')\n",
        "KDTreePruningKMeans = jpype.JClass('elki.clustering.kmeans.KDTreePruningKMeans')\n",
        "KMeansMinusMinus = jpype.JClass('elki.clustering.kmeans.KMeansMinusMinus')\n",
        "KMediansLloyd = jpype.JClass('elki.clustering.kmeans.KMediansLloyd')\n",
        "MacQueenKMeans = jpype.JClass('elki.clustering.kmeans.MacQueenKMeans')\n",
        "SimplifiedElkanKMeans = jpype.JClass('elki.clustering.kmeans.SimplifiedElkanKMeans')\n",
        "SingleAssignmentKMeans = jpype.JClass('elki.clustering.kmeans.SingleAssignmentKMeans')\n",
        "XMeans = jpype.JClass('elki.clustering.kmeans.XMeans')\n",
        "SortMeans = jpype.JClass('elki.clustering.kmeans.SortMeans')\n",
        "ParallelLloydKMeans = jpype.JClass('elki.clustering.kmeans.parallel.ParallelLloydKMeans')\n",
        "KMeansQualityMeasure = jpype.JClass('elki.clustering.kmeans.quality.KMeansQualityMeasure')\n",
        "SimplifiedElkanKMeans = jpype.JClass('elki.clustering.kmeans.SimplifiedElkanKMeans')\n",
        "WithinClusterVariance = JClass(\"elki.clustering.kmeans.quality.WithinClusterVariance\")\n",
        "KDTreePruningKMeansSplit = JClass(\"elki.clustering.kmeans.KDTreePruningKMeans$Split\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_file(file):\n",
        "    print(file)\n",
        "    df = pd.read_csv(file)  # Use the file variable from the loop\n",
        "    # Drop the first row (which might contain labels or bad data)\n",
        "    df = df.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    # Assume the last column is the target/label column\n",
        "    true_labels = df.iloc[:, -1].values  # Extract labels from last column\n",
        "    df = df.iloc[:, :-1]                 # Drop last column (features only remain)\n",
        "\n",
        "    # Count number of unique labels\n",
        "    k_centroids = len(np.unique(true_labels))\n",
        "    print(f\"k = {k_centroids}\")\n",
        "    \"\"\"\n",
        "    # Extract ground truth labels if available\n",
        "    if 'target' in df.columns:\n",
        "        true_labels = df['target'].values  # Ground truth labels for evaluation\n",
        "        df = df.drop(columns=['target'])   # Drop label column before clustering\n",
        "        df = df.drop(index=0).reset_index(drop=True)\n",
        "        k_centroids =len(np.unique(true_labels))\n",
        "        print(f\"k=\",k_centroids)\n",
        "    else:\n",
        "        true_labels = None\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure only numeric columns are used\n",
        "    data_values = df.select_dtypes(include=['number']).values.astype(float)\n",
        "\n",
        "    # Convert to 2D Java array (double[][])\n",
        "    java_data = jpype.JArray(JDouble, 2)(data_values)\n",
        "\n",
        "    # Prepare ELKI database\n",
        "    adapter = ArrayAdapterDatabaseConnection(java_data)\n",
        "    database = StaticArrayDatabase(adapter, [])\n",
        "    database.initialize()\n",
        "\n",
        "    # Find the correct relation of type NumberVector\n",
        "    relation = None\n",
        "    for rel in database.getRelations():\n",
        "        if NumberVector.class_.isAssignableFrom(rel.getDataTypeInformation().getRestrictionClass()):\n",
        "            relation = rel\n",
        "            break\n",
        "\n",
        "    if relation is None:\n",
        "        raise ValueError(\"No valid Relation<NumberVector> found in the database!\")\n",
        "\n",
        "    # Debugging: Print the number of relations in the database\n",
        "    #print(\"Total Relations in Database:\", len(database.getRelations()))\n",
        "\n",
        "    # Debugging: Check if relation exists and print details\n",
        "    if relation:\n",
        "        #print(\"✅ Relation successfully found!\")\n",
        "        #print(\"Total Data Points in Relation:\", relation.size())\n",
        "\n",
        "        # Fetch a few data points from relation (convert from ELKI to Python)\n",
        "        iter_dbid = relation.iterDBIDs()  # Get all IDs\n",
        "        sample_points = []\n",
        "        for _ in range(5):  # Print first 5 points\n",
        "            if not iter_dbid.valid():\n",
        "                break  # Stop if there are no more data points\n",
        "            obj = relation.get(iter_dbid)\n",
        "            sample_points.append([obj.doubleValue(i) for i in range(obj.getDimensionality())])\n",
        "            iter_dbid.advance()\n",
        "\n",
        "    return relation, data_values, true_labels, k_centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_centroids_pos(result):\n",
        "    centroids = []\n",
        "    for cluster in result.getAllClusters():\n",
        "        if not cluster.isNoise():\n",
        "            model = cluster.getModel()  # ✅ Extract KMeansModel\n",
        "            if model is not None:\n",
        "                centroid = centroid = list(model.getMean()) \n",
        "                centroids.append(centroid)  # ✅ Store centroid\n",
        "    centroids = np.array(centroids)\n",
        "    return centroids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clustering(relation,data_values,kmeans):     \n",
        "   \n",
        "    \n",
        "    \n",
        "    # Start the timer (tic)\n",
        "    start_time = time.perf_counter()\n",
        "    result = kmeans.run(relation)\n",
        "\n",
        "    end_time = time.perf_counter()\n",
        "    clustering_time = end_time - start_time\n",
        "\n",
        "    # Extract cluster assignments\n",
        "    cluster_labels = np.full(len(data_values), -1)  # Initialize with -1 (unassigned)\n",
        "    # Build the cluster labels array based on the relation's DBIDs.\n",
        "    dbid_range = relation.getDBIDs()        # This is an IntegerDBIDRange for all data points.\n",
        "    num_dbids = dbid_range.size()             # Should be 150 for the iris dataset.\n",
        "    cluster_labels = np.full(num_dbids, -1)   # Initialize with -1 (unassigned)\n",
        "\n",
        "    for cluster_id, cluster in enumerate(result.getAllClusters()):\n",
        "        #print(\"Processing cluster_ID:\", cluster_id)\n",
        "        if not cluster.isNoise():\n",
        "            # Loop over all DBIDs (in the same order as the data_values)\n",
        "            for i in range(num_dbids):\n",
        "                dbid = dbid_range.get(i)  # Get the DBID corresponding to the i-th data point.\n",
        "                # Check if this DBID is in the cluster's set.\n",
        "                if cluster.getIDs().contains(dbid):\n",
        "                    cluster_labels[i] = cluster_id\n",
        "                    #print(f\"Assigned cluster {cluster_id} to index {i}\")\n",
        "\n",
        "    centroids_pos = get_centroids_pos(result)\n",
        "    \n",
        "    return result, cluster_labels,centroids_pos, clustering_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_inertia(relation, result):\n",
        "    SquaredErrors = jpype.JClass('elki.evaluation.clustering.internal.SquaredErrors')\n",
        "    NoiseHandling = jpype.JClass('elki.evaluation.clustering.internal.NoiseHandling')\n",
        "    available_constants = [const.name() for const in NoiseHandling.values()]\n",
        "    #print(\"Available NoiseHandling constants:\", available_constants)\n",
        "\n",
        "    # Suppose you have:\n",
        "    #   database:    a StaticArrayDatabase\n",
        "    #   relation:    a Relation<NumberVector>\n",
        "    #   clustering:  the result of LloydKMeans.run(relation)\n",
        "\n",
        "    # 1. Choose a distance and noise handling\n",
        "    distance = EuclideanDistance()\n",
        "    # List all enum values if you're unsure which is available:\n",
        "    #   print([v.name() for v in NoiseHandling.values()])\n",
        "    noise_handling = NoiseHandling.valueOf(\"MERGE_NOISE\")  # or another valid enum\n",
        "\n",
        "    # 2. Create the evaluator\n",
        "    sse_evaluator = SquaredErrors(distance, noise_handling)\n",
        "\n",
        "    # 3. Compute SSE (sum of squared errors)\n",
        "    sse = sse_evaluator.evaluateClustering(relation, result)\n",
        "    return sse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_accuracy(true_labels, predicted_labels):\n",
        "    if true_labels is None:\n",
        "         return None\n",
        "    # Create a contingency (confusion) matrix\n",
        "    C = confusion_matrix(true_labels, predicted_labels)\n",
        "    # print(C)\n",
        "    # Use the Hungarian algorithm to maximize the total correct assignments\n",
        "    #row_ind and col_ind are 2 numpy array\n",
        "    row_ind, col_ind = linear_sum_assignment(-C)  # We use negative because we want to maximize\n",
        "    #print(\"row_ind \", row_ind)\n",
        "    #print(\"col_ind \",col_ind)\n",
        "    # Sum the counts from the optimal assignment\n",
        "    total_correct = C[row_ind, col_ind].sum()\n",
        "    #print(\"total_correct \", total_correct)\n",
        "    # Calculate accuracy as the ratio of correctly assigned samples\n",
        "    accuracy = total_correct / np.sum(C)\n",
        "    #print (\"sum(C) \", np.sum(C))\n",
        "    #print(accuracy)\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_evaluation_scores(true_labels, predicted_labels):\n",
        "    nmi_score = normalized_mutual_info_score(true_labels, predicted_labels) if true_labels is not None else None\n",
        "    accuracy_score= compute_accuracy(true_labels, predicted_labels)\n",
        "    print(f\"NMI: \",nmi_score)\n",
        "    print(f\"accuracy_score: \",accuracy_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def choose_best_algorithm_and_write_centroids(algorithms_map, output_file=\"centroids.txt\"):\n",
        "    \"\"\"\n",
        "    Evaluates a dictionary of algorithms (each mapped to a tuple of \n",
        "    (time, accuracy, centroids)) to find the best algorithm based on \n",
        "    the lowest time/accuracy ratio. After picking the best algorithm, \n",
        "    this function writes the positions of its k centroids into a text file.\n",
        "\n",
        "    Parameters:\n",
        "        algorithms_map (dict): Dictionary where each key is an algorithm name and \n",
        "                               each value is a tuple (time, accuracy, centroids).\n",
        "                               For example:\n",
        "                               {\n",
        "                                   \"Algorithm A\": (120, 0.85, [(x1, y1), (x2, y2), ...]),\n",
        "                                   \"Algorithm B\": (100, 0.80, [(x1, y1), (x2, y2), ...]),\n",
        "                                   ...\n",
        "                               }\n",
        "        output_file (str): The file path to write the centroids' positions (default \"centroids.txt\").\n",
        "\n",
        "    Returns:\n",
        "        tuple: (best_algo_name, best_ratio) where best_algo_name is the name of the best algorithm,\n",
        "               and best_ratio is its time/accuracy ratio.\n",
        "    \"\"\"\n",
        "    best_algo_name = None\n",
        "    best_ratio = 0\n",
        "    \n",
        "    # Iterate over each algorithm to compute the time/accuracy ratio.\n",
        "    for algo_name, metrics in algorithms_map.items():\n",
        "        # Since metrics is a tuple: (time, accuracy, centroids)\n",
        "        time_val, accuracy_val, centroids = metrics\n",
        "        \n",
        "        # Skip algorithms with missing metrics or accuracy of zero.\n",
        "        if time_val is None or accuracy_val is None or accuracy_val == 0:\n",
        "            continue\n",
        "        \n",
        "        ratio = accuracy_val\n",
        "        print(f\"Algorithm: {algo_name}, Time: {time_val}, Accuracy: {accuracy_val}, Ratio: {ratio:.3f}\")\n",
        "        \"\"\" what is this find max :)))))\n",
        "        if ratio < best_ratio:\n",
        "            best_ratio = ratio\n",
        "            best_algo_name = algo_name\n",
        "        \"\"\"\n",
        "        if ratio > best_ratio:\n",
        "            best_ratio=ratio \n",
        "            best_algo_name = algo_name\n",
        "            \n",
        "    if best_algo_name is None:\n",
        "        print(\"No valid algorithm found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Retrieve centroids from the best algorithm's tuple.\n",
        "    _, _, centroids_best = algorithms_map[best_algo_name]\n",
        "    \n",
        "    if centroids_best is None:\n",
        "        print(\"Best algorithm does not contain centroid information.\")\n",
        "        return best_algo_name, best_ratio\n",
        "\n",
        "    # Write the centroid positions to the specified text file.\n",
        "    with open(output_file, \"w\") as file:\n",
        "        #file.write(f\"Centroids for {best_algo_name} (Ratio: {best_ratio:.3f}):\\n\")\n",
        "        for idx, centroid in enumerate(centroids_best, start=1):\n",
        "            file.write(f\"{centroid}\\n\")\n",
        "    \n",
        "    print(f\"Centroid positions written to {output_file}\")\n",
        "    print(f\"\\nThe best algorithm is {best_algo_name} with a time/accuracy ratio of {best_ratio:.3f}.\")\n",
        "    return best_algo_name, best_ratio\n",
        "\n",
        "\n",
        "\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_iter=300 \n",
        "algorithm_constructors = [\n",
        "        lambda k_centroids: LloydKMeans(EuclideanDistance(), k_centroids, max_iter, RandomlyChosen(RandomFactory(42))),\n",
        "        lambda k_centroids: LloydKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: ElkanKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: SimplifiedElkanKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: YinYangKMeans(k_centroids, max_iter, RandomlyChosen(RandomFactory(42)), 5),\n",
        "        lambda k_centroids: AnnulusKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: HamerlyKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: ShallotKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: ExponionKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: BestOfMultipleKMeans(10,LloydKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),wcss()),\n",
        "        lambda k_centroids: CompareMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: FuzzyCMeans(k_centroids, 10, max_iter, 0.0001, 2,False, KMeansPlusPlus(RandomFactory(42)) ),             \n",
        "        lambda k_centroids: HartiganWongKMeans( k_centroids,KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: KDTreeFilteringKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), KDTreePruningKMeans.Split.SSQ,40),\n",
        "        lambda k_centroids: KMeansMinusMinus(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)),0.05, True),\n",
        "        lambda k_centroids: KDTreePruningKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), KDTreePruningKMeansSplit.SSQ,40),\n",
        "        lambda k_centroids: KMediansLloyd(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: MacQueenKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: SimplifiedElkanKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True),\n",
        "        lambda k_centroids: SingleAssignmentKMeans(EuclideanDistance(), k_centroids,KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: SortMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),\n",
        "        lambda k_centroids: ParallelLloydKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)))\n",
        "]\n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nlambda k_centroids: GMeans(EuclideanDistance(),0.05, k_centroids, k_centroids,max_iter,\\n                                AnnulusKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True), \\n                                KMeansPlusPlus(RandomFactory(42)), RandomFactory(42)),\\n lambda k_centroids: XMeans(\\n        EuclideanDistance(),  # Distance function\\n        k_centroids,  # Minimum number of clusters\\n        k_centroids * 2,  # Maximum number of clusters (XMeans can return up to 2*k)\\n        max_iter,  # Maximum iterations\\n        LloydKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),  # Inner K-Means variant\\n        KMeansPlusPlus(RandomFactory(42)),  # Initialization method\\n        WithinClusterVariance(),  # Correct quality measure\\n        RandomFactory(42)  # Random factory\\n        ),\\n'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "lambda k_centroids: GMeans(EuclideanDistance(),0.05, k_centroids, k_centroids,max_iter,\n",
        "                                AnnulusKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42)), True), \n",
        "                                KMeansPlusPlus(RandomFactory(42)), RandomFactory(42)),\n",
        " lambda k_centroids: XMeans(\n",
        "        EuclideanDistance(),  # Distance function\n",
        "        k_centroids,  # Minimum number of clusters\n",
        "        k_centroids * 2,  # Maximum number of clusters (XMeans can return up to 2*k)\n",
        "        max_iter,  # Maximum iterations\n",
        "        LloydKMeans(EuclideanDistance(), k_centroids, max_iter, KMeansPlusPlus(RandomFactory(42))),  # Inner K-Means variant\n",
        "        KMeansPlusPlus(RandomFactory(42)),  # Initialization method\n",
        "        WithinClusterVariance(),  # Correct quality measure\n",
        "        RandomFactory(42)  # Random factory\n",
        "        ),\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compare_runtime/processed_dataset_44682.csv\n",
            "k = 2\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@3c153a1, Time: 0.00519998898380436, Accuracy: 0.5062531265632816, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@3745e5c6, Time: 0.011105595011031255, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.ElkanKMeans@4387b79e, Time: 0.025512388994684443, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@527e5409, Time: 0.02005980201647617, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.YinYangKMeans@7ea37dbf, Time: 0.018792848975863308, Accuracy: 0.5237618809404703, Ratio: 0.524\n",
            "Algorithm: elki.clustering.kmeans.AnnulusKMeans@27c86f2d, Time: 0.023639310005819425, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.HamerlyKMeans@1df82230, Time: 0.019283495988929644, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.ShallotKMeans@7995092a, Time: 0.018273461988428608, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.ExponionKMeans@2133814f, Time: 0.015320798993343487, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.BestOfMultipleKMeans@16e7dcfd, Time: 0.04239189997315407, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.CompareMeans@14d3bc22, Time: 0.01600417500594631, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.FuzzyCMeans@4df50bcc, Time: 0.04327816399745643, Accuracy: 0.5067533766883442, Ratio: 0.507\n",
            "Algorithm: elki.clustering.kmeans.HartiganWongKMeans@6dbb137d, Time: 0.009664832992712036, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.KDTreeFilteringKMeans@75329a49, Time: 0.2292042479966767, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.KMeansMinusMinus@6af93788, Time: 0.023724459984805435, Accuracy: 0.4797398699349675, Ratio: 0.480\n",
            "Algorithm: elki.clustering.kmeans.KDTreePruningKMeans@5be6e01c, Time: 0.2010290239995811, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.KMediansLloyd@78b66d36, Time: 0.06954080000286922, Accuracy: 0.5052526263131566, Ratio: 0.505\n",
            "Algorithm: elki.clustering.kmeans.MacQueenKMeans@6fe7aac8, Time: 0.0044627739989664406, Accuracy: 0.5062531265632816, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@2473d930, Time: 0.0065893410064745694, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.SingleAssignmentKMeans@5ae50ce6, Time: 0.0008893549966160208, Accuracy: 0.5307653826913457, Ratio: 0.531\n",
            "Algorithm: elki.clustering.kmeans.SortMeans@48e4374, Time: 0.018291721993591636, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.parallel.ParallelLloydKMeans@6b927fb, Time: 0.3785909569996875, Accuracy: 0.5057528764382191, Ratio: 0.506\n",
            "Centroid positions written to compare_runtime_output/centroids_processed_dataset_44682.csv\n",
            "\n",
            "The best algorithm is elki.clustering.kmeans.SingleAssignmentKMeans@5ae50ce6 with a time/accuracy ratio of 0.531.\n",
            "Best centroids saved to compare_runtime_output/centroids_processed_dataset_44682.csv\n",
            "compare_runtime/processed_dataset_46482.csv\n",
            "k = 2\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@4cc451f2, Time: 0.002024178975261748, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@9f116cc, Time: 0.0030348919972311705, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.ElkanKMeans@5876a9af, Time: 0.004372915020212531, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@6b81ce95, Time: 0.005072859989013523, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.YinYangKMeans@37afeb11, Time: 0.002525385993067175, Accuracy: 0.5355355355355356, Ratio: 0.536\n",
            "Algorithm: elki.clustering.kmeans.AnnulusKMeans@6c64cb25, Time: 0.00926492500002496, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.HamerlyKMeans@16293aa2, Time: 0.007585675019072369, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.ShallotKMeans@2d7275fc, Time: 0.013194481987738982, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.ExponionKMeans@79e2c065, Time: 0.009581054007867351, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.BestOfMultipleKMeans@49139829, Time: 0.02151793401571922, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.CompareMeans@1b7cc17c, Time: 0.004978359997039661, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.FuzzyCMeans@1a482e36, Time: 0.008345191017724574, Accuracy: 0.5145145145145145, Ratio: 0.515\n",
            "Algorithm: elki.clustering.kmeans.HartiganWongKMeans@747ddf94, Time: 0.004601473017828539, Accuracy: 0.5215215215215215, Ratio: 0.522\n",
            "Algorithm: elki.clustering.kmeans.KDTreeFilteringKMeans@4f83df68, Time: 0.02182521199574694, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.KMeansMinusMinus@293a5bf6, Time: 0.010547198005951941, Accuracy: 0.48148148148148145, Ratio: 0.481\n",
            "Algorithm: elki.clustering.kmeans.KDTreePruningKMeans@f6efaab, Time: 0.021696611976949498, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.KMediansLloyd@38d8f54a, Time: 0.0765170090016909, Accuracy: 0.5025025025025025, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.MacQueenKMeans@55b699ef, Time: 0.0016962689987849444, Accuracy: 0.6706706706706707, Ratio: 0.671\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@799f10e1, Time: 0.004768790997331962, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.SingleAssignmentKMeans@313ac989, Time: 0.00030711901490576565, Accuracy: 0.5865865865865866, Ratio: 0.587\n",
            "Algorithm: elki.clustering.kmeans.SortMeans@6302bbb1, Time: 0.005068169994046912, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.parallel.ParallelLloydKMeans@179ece50, Time: 0.10583781599416398, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Centroid positions written to compare_runtime_output/centroids_processed_dataset_46482.csv\n",
            "\n",
            "The best algorithm is elki.clustering.kmeans.MacQueenKMeans@55b699ef with a time/accuracy ratio of 0.671.\n",
            "Best centroids saved to compare_runtime_output/centroids_processed_dataset_46482.csv\n",
            "compare_runtime/processed_dataset_46532.csv\n",
            "k = 2\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@74e52ef6, Time: 0.0043245940178167075, Accuracy: 0.5175175175175175, Ratio: 0.518\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@489115ef, Time: 0.001815359981264919, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.ElkanKMeans@3c9754d8, Time: 0.003410509991226718, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@4b2bac3f, Time: 0.0019355880212970078, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.YinYangKMeans@302552ec, Time: 0.0028630629822146147, Accuracy: 0.5465465465465466, Ratio: 0.547\n",
            "Algorithm: elki.clustering.kmeans.AnnulusKMeans@367ffa75, Time: 0.0025510150007903576, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.HamerlyKMeans@55fe41ea, Time: 0.0022433470003306866, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.ShallotKMeans@7a8c8dcf, Time: 0.002689343993552029, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.ExponionKMeans@3a52dba3, Time: 0.0022171369928400964, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.BestOfMultipleKMeans@131ef10, Time: 0.024342587013961747, Accuracy: 0.5175175175175175, Ratio: 0.518\n",
            "Algorithm: elki.clustering.kmeans.CompareMeans@23348b5d, Time: 0.0018260689976159483, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.FuzzyCMeans@20d525, Time: 0.005554166971705854, Accuracy: 0.5055055055055055, Ratio: 0.506\n",
            "Algorithm: elki.clustering.kmeans.HartiganWongKMeans@6f4a47c7, Time: 0.0035436690086498857, Accuracy: 0.5275275275275275, Ratio: 0.528\n",
            "Algorithm: elki.clustering.kmeans.KDTreeFilteringKMeans@4009e306, Time: 0.01765407601487823, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.KMeansMinusMinus@63475ace, Time: 0.007802723994245753, Accuracy: 0.4774774774774775, Ratio: 0.477\n",
            "Algorithm: elki.clustering.kmeans.KDTreePruningKMeans@6913c1fb, Time: 0.017885875015053898, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.KMediansLloyd@17f7cd29, Time: 0.06599819101393223, Accuracy: 0.5675675675675675, Ratio: 0.568\n",
            "Algorithm: elki.clustering.kmeans.MacQueenKMeans@3c3d9b6b, Time: 0.0055556269944645464, Accuracy: 0.5175175175175175, Ratio: 0.518\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@4e50c791, Time: 0.002206386998295784, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.SingleAssignmentKMeans@394df057, Time: 0.000214739004150033, Accuracy: 0.6696696696696697, Ratio: 0.670\n",
            "Algorithm: elki.clustering.kmeans.SortMeans@33c911a1, Time: 0.002693245012778789, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.parallel.ParallelLloydKMeans@36d585c, Time: 0.06103216001065448, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Centroid positions written to compare_runtime_output/centroids_processed_dataset_46532.csv\n",
            "\n",
            "The best algorithm is elki.clustering.kmeans.SingleAssignmentKMeans@394df057 with a time/accuracy ratio of 0.670.\n",
            "Best centroids saved to compare_runtime_output/centroids_processed_dataset_46532.csv\n",
            "compare_runtime/processed_dataset_46416.csv\n",
            "k = 2\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@1356d4d4, Time: 0.002027657988946885, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.LloydKMeans@41c2284a, Time: 0.0029845530225429684, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.ElkanKMeans@4f67eb2a, Time: 0.005286198982503265, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@8317c52, Time: 0.0023044670233502984, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.YinYangKMeans@6c779568, Time: 0.002410785004030913, Accuracy: 0.5355355355355356, Ratio: 0.536\n",
            "Algorithm: elki.clustering.kmeans.AnnulusKMeans@2e1d27ba, Time: 0.003513449977617711, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.HamerlyKMeans@152aa092, Time: 0.002963422011816874, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.ShallotKMeans@37858383, Time: 0.0031521210039500147, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.ExponionKMeans@2beee7ff, Time: 0.00296490199980326, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.BestOfMultipleKMeans@1f97cf0d, Time: 0.02042188899940811, Accuracy: 0.5205205205205206, Ratio: 0.521\n",
            "Algorithm: elki.clustering.kmeans.CompareMeans@7690781, Time: 0.0026933749904856086, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.FuzzyCMeans@61386958, Time: 0.007423536997521296, Accuracy: 0.5145145145145145, Ratio: 0.515\n",
            "Algorithm: elki.clustering.kmeans.HartiganWongKMeans@6b8ca3c8, Time: 0.004286724986741319, Accuracy: 0.5215215215215215, Ratio: 0.522\n",
            "Algorithm: elki.clustering.kmeans.KDTreeFilteringKMeans@34f7cfd9, Time: 0.020854656992014498, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.KMeansMinusMinus@55536d9e, Time: 0.0037241980026010424, Accuracy: 0.48148148148148145, Ratio: 0.481\n",
            "Algorithm: elki.clustering.kmeans.KDTreePruningKMeans@2cd2a21f, Time: 0.021299325017025694, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.KMediansLloyd@378542de, Time: 0.07853607699507847, Accuracy: 0.5025025025025025, Ratio: 0.503\n",
            "Algorithm: elki.clustering.kmeans.MacQueenKMeans@c333c60, Time: 0.001001503987936303, Accuracy: 0.6706706706706707, Ratio: 0.671\n",
            "Algorithm: elki.clustering.kmeans.SimplifiedElkanKMeans@72cde7cc, Time: 0.0026743840135168284, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.SingleAssignmentKMeans@53976f5c, Time: 0.00026967801386490464, Accuracy: 0.5865865865865866, Ratio: 0.587\n",
            "Algorithm: elki.clustering.kmeans.SortMeans@41e1e210, Time: 0.0031048709934111685, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Algorithm: elki.clustering.kmeans.parallel.ParallelLloydKMeans@6da21078, Time: 0.078258009016281, Accuracy: 0.5395395395395396, Ratio: 0.540\n",
            "Centroid positions written to compare_runtime_output/centroids_processed_dataset_46416.csv\n",
            "\n",
            "The best algorithm is elki.clustering.kmeans.MacQueenKMeans@c333c60 with a time/accuracy ratio of 0.671.\n",
            "Best centroids saved to compare_runtime_output/centroids_processed_dataset_46416.csv\n"
          ]
        }
      ],
      "source": [
        "def main(input_dir, output_dir=\"results\"):\n",
        "    # Create results directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "\n",
        "    # Automatically find all CSV files in the provided directory.\n",
        "    csv_files = glob.glob(os.path.join(input_dir, '*.csv'))\n",
        "    if not csv_files:\n",
        "        print(\"No CSV files found in the specified directory.\")\n",
        "        return\n",
        "    \n",
        "    # Process each CSV file individually\n",
        "\n",
        "    for file in csv_files:\n",
        "        #process the file \n",
        "\n",
        "        #uncomment to mass process\n",
        "        relation,data_values,true_labels,k_centroids = process_file(file)\n",
        "\n",
        "        # uncomment to run any single dataset\n",
        "        #relation,data_values,true_labels,k_centroids = process_file(\"/home/esrp2024/tmp/train/train_dataset_1485.csv\")\n",
        "        \n",
        "        #list of algors\n",
        "        #algorithms_list=[\"LloydKMeans\",\"KMeansPlusPlus\",\"ElkanKmeans\",\"YinYangKMeans\",\"AnnulusKMeans\",\"HamerlyKMeans\"]\n",
        "        #empty map\n",
        "        algorithms_map = {}\n",
        "\n",
        "        #for algorithm_name in algorithms_list:\n",
        "        for constructor in algorithm_constructors:\n",
        "            kmeans = constructor(k_centroids)\n",
        "            # Get the base name of the file (i.e., the file name without the path)\n",
        "            #basename = os.path.basename(file)\n",
        "            \n",
        "            \n",
        "            \n",
        "            result,predicted_labels,centroids_pos,clustering_time = clustering(relation, data_values,kmeans)\n",
        "            #inertia=calculate_inertia(relation, result)\n",
        "            acc=compute_accuracy(true_labels, predicted_labels)\n",
        "            \n",
        "            #map\n",
        "            algorithms_map[kmeans] = (clustering_time, acc, centroids_pos)\n",
        "            \n",
        "         # Generate a unique output file for each dataset inside the \"results\" folder\n",
        "        dataset_name = os.path.basename(file).replace('.csv', '')  # Extract dataset name\n",
        "        centroids_filename = os.path.join(output_dir, f\"centroids_{dataset_name}.csv\")  # Save best centroids\n",
        "\n",
        "        # Choose the best algorithm and write only the best centroids to a dataset-specific file\n",
        "        choose_best_algorithm_and_write_centroids(algorithms_map, output_file=centroids_filename)\n",
        "\n",
        "        print(f\"Best centroids saved to {centroids_filename}\")\n",
        "\n",
        "        \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description='Process multiple CSV datasets from a directory.')\n",
        "    parser.add_argument('--input_dir', type=str, default='train_no_header_large', \n",
        "                        help='Directory containing CSV files (default is \"sub_sampling_cv(nick)\")')\n",
        "    parser.add_argument('--output_dir', type=str, default='compare_runtime_output', \n",
        "                        help='Directory to save the results (default is \"results\")')\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    main(args.input_dir, args.output_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed: processed_dataset_1496.csv\n",
            "Processed: processed_dataset_44130.csv\n",
            "Processed: processed_dataset_44124.csv\n",
            "Processed: processed_dataset_1507.csv\n",
            "Processed: processed_dataset_46504.csv\n",
            "Processed: processed_dataset_41146.csv\n",
            "Processed: processed_dataset_46483.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Input and output folders\n",
        "input_folder = \"train_w_header_large\"  # Folder containing CSV files\n",
        "output_folder = \"train_no_header_large\"\n",
        "#output_folder2 = \"test_w_exact_true_labels\"\n",
        "# Create output folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Process all .csv files in the input folder\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        input_path = os.path.join(input_folder, filename)\n",
        "        output_path = os.path.join(output_folder, filename)\n",
        "        #true_labels_path = os.path.join(output_folder2, \"exact_true_labels_\" + filename)\n",
        "        # Read without assuming header\n",
        "        df = pd.read_csv(input_path, header=None)\n",
        "\n",
        "        # Drop first row and last column\n",
        "        df = df.drop(index=0).reset_index(drop=True)\n",
        "        true_labels = df.iloc[:, -1].values  # Extract labels from last column\n",
        "        df = df.iloc[:, :-1]\n",
        "\n",
        "        # Save to output folder with the same filename\n",
        "        df.to_csv(output_path, index=False, header=False)\n",
        "        #pd.Series(true_labels).to_csv(true_labels_path, index=False, header=False)\n",
        "        print(f\"Processed: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV files found: 45\n"
          ]
        }
      ],
      "source": [
        "def count_csv_files(folder_path):\n",
        "    return sum(1 for file in os.listdir(folder_path) if file.endswith('.csv'))\n",
        "folder = \"/home/esrp2024/tmp/results_for_test_data\"\n",
        "print(f\"CSV files found: {count_csv_files(folder)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (base)",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
