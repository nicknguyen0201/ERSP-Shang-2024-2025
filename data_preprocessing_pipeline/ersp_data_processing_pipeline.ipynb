{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install openml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY7SCbm_n6zm",
        "outputId": "f02dbf53-c7cd-46a6-94c7-26f96b832e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openml\n",
            "  Downloading openml-0.15.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting liac-arff>=2.4.0 (from openml)\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xmltodict (from openml)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from openml) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from openml) (1.6.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from openml) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from openml) (2.2.2)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.11/dist-packages (from openml) (1.15.2)\n",
            "Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from openml) (2.0.2)\n",
            "Collecting minio (from openml)\n",
            "  Downloading minio-7.2.15-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from openml) (18.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openml) (4.67.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from openml) (24.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->openml) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.0->openml) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->openml) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->openml) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->openml) (3.6.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from minio->openml) (2025.4.26)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from minio->openml) (2.4.0)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from minio->openml) (23.1.0)\n",
            "Collecting pycryptodome (from minio->openml)\n",
            "  Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from minio->openml) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->openml) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->openml) (3.10)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->minio->openml) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio->openml) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio->openml) (2.22)\n",
            "Downloading openml-0.15.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading minio-7.2.15-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading pycryptodome-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11717 sha256=264651e5a023f1bec50f22d05052a02014ddc143c105409a02cb35aa5caed214\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/23/31/5e562fce1f95aabe57f2a7320d07433ba1cd152bcde2f6a002\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: xmltodict, pycryptodome, liac-arff, minio, openml\n",
            "Successfully installed liac-arff-2.5.0 minio-7.2.15 openml-0.15.1 pycryptodome-3.22.0 xmltodict-0.14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "folder_path = \"/content/data/processed\"\n",
        "\n",
        "if os.path.exists(folder_path):\n",
        "    shutil.rmtree(folder_path)\n",
        "    print(f\" Deleted folder: {folder_path}\")\n",
        "else:\n",
        "    print(f\" Folder not found: {folder_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGXZl5-rWVuy",
        "outputId": "b65f7543-1fde-4169-c2dc-fe60dcf92977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Folder not found: /content/data/processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openml\n",
        "import pandas as pd\n",
        "\n",
        "# List all datasets\n",
        "datasets = openml.datasets.list_datasets(output_format='dataframe')\n",
        "\n",
        "# Filter for classification datasets with ~20 raw features and manageable size\n",
        "filtered = datasets[\n",
        "    (datasets['NumberOfFeatures'] <= 21) &\n",
        "    (datasets['NumberOfFeatures'] > 8) & # Allow some wiggle room\n",
        "    (datasets['NumberOfClasses'].notna()) &\n",
        "    (datasets['NumberOfInstances'] > 3000) &\n",
        "    (datasets['NumberOfInstances'] <= 5000)\n",
        "]\n",
        "\n",
        "# Display key info\n",
        "id_list = filtered['did'].tolist()\n",
        "print(f\"Selected {len(id_list)} datasets:\")\n",
        "print(id_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "5I8fVf0xSz6s",
        "outputId": "a0fbd82e-4294-4238-d390-bbf9a550a666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a2f94c017723>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# List all datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dataframe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Filter for classification datasets with ~20 raw features and manageable size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36mlist_datasets\u001b[0;34m(data_id, offset, size, status, tag, output_format, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     return openml.utils._list_all(  # type: ignore\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mdata_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mlist_output_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_format\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openml/utils.py\u001b[0m in \u001b[0;36m_list_all\u001b[0;34m(listing_call, list_output_format, *args, **filters)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mcurrent_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE_ORIG\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             new_batch = listing_call(\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0moutput_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_output_format\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36m_list_datasets\u001b[0;34m(data_id, output_format, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdata_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mapi_call\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"/data_id/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m__list_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openml/datasets/functions.py\u001b[0m in \u001b[0;36m__list_datasets\u001b[0;34m(api_call, output_format)\u001b[0m\n\u001b[1;32m    271\u001b[0m ) -> dict | pd.DataFrame:\n\u001b[1;32m    272\u001b[0m     \u001b[0mxml_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api_calls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mdatasets_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxmltodict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oml:dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;31m# Minimalistic check if the XML is useful\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xmltodict.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(xml_input, encoding, expat, process_namespaces, namespace_separator, disable_entities, process_comments, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m../Modules/pyexpat.c\u001b[0m in \u001b[0;36mCharacterData\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xmltodict.py\u001b[0m in \u001b[0;36mcharacters\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mcharacters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs('/content/data/train', exist_ok=True)\n",
        "os.makedirs('/content/data/test', exist_ok=True)"
      ],
      "metadata": {
        "id": "HPeo046ByWTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openml\n",
        "import pandas as pd\n",
        "# import category_encoders as ce\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class OpenMLPipeline:\n",
        "    def __init__(self, dataset_id, test_size=0.2, random_state=42):\n",
        "        self.dataset_id = dataset_id\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        self.dataset = None\n",
        "        self.features = None\n",
        "        self.labels = None\n",
        "        self.preprocessor = None\n",
        "\n",
        "    def fetch_data(self):\n",
        "        print(f\"Fetching dataset {self.dataset_id} from OpenML...\")\n",
        "        dataset = openml.datasets.get_dataset(self.dataset_id)\n",
        "        X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "        # Convert target to numerical if it's not\n",
        "        if self.labels.dtype == 'object' or self.labels.dtype.name == 'category' or self.labels.dtype.name == 'bool':\n",
        "          print(\"Target is categorical, encoding to numerical...\")\n",
        "          self.labels = LabelEncoder().fit_transform(self.labels)\n",
        "\n",
        "        print(\"fetch data:\", X.shape)\n",
        "        return X, y\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        if self.features is None or self.labels is None:\n",
        "            raise ValueError(\"Dataset not loaded. Call fetch_data() first.\")\n",
        "\n",
        "        numeric_features = self.features.select_dtypes(include=['int64', 'float64', 'uint8']).columns\n",
        "        categorical_features = self.features.select_dtypes(include=['object', 'category', 'bool']).columns\n",
        "\n",
        "        if len(numeric_features) + len(categorical_features) != self.features.shape[1]:\n",
        "            print(\"Some features are neither categorized as numeric nor categorical. Skipping this dataset.\")\n",
        "            return None, None\n",
        "\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='mean')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "\n",
        "        self.preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        X_processed = self.preprocessor.fit_transform(self.features)\n",
        "        # print(\"Feature Data Types:\\n\", self.features.dtypes)\n",
        "        # print(\"Original features:\", list(self.features.columns))\n",
        "        # print(\"Numeric features used:\", list(numeric_features))\n",
        "        # print(\"Categorical features used:\", list(categorical_features))\n",
        "        # print(\"Preprocessed Data Shape:\", X_processed.shape)\n",
        "\n",
        "        if isinstance(X_processed, pd.DataFrame):\n",
        "            X_processed.columns = self.preprocessor.get_feature_names_out()\n",
        "        else:\n",
        "            X_processed = pd.DataFrame(X_processed, columns=self.preprocessor.get_feature_names_out())\n",
        "        return X_processed, self.labels\n",
        "\n",
        "    def split_data(self, X, y):\n",
        "        return train_test_split(X, y, test_size=self.test_size, random_state=self.random_state)\n",
        "\n",
        "    def save_to_csv(self, X, y, X_train, X_test, y_train, y_test):\n",
        "        # df_full = pd.DataFrame(X)\n",
        "        # df_full['target'] = y.values\n",
        "        # df_full.to_csv(f\"processed_dataset_{self.dataset_id}.csv\", index=False)\n",
        "\n",
        "        df_train = pd.DataFrame(X_train)\n",
        "        df_train['target'] = y_train\n",
        "        df_train.to_csv(f\"/content/data/train/train_dataset_{self.dataset_id}.csv\", index=False)\n",
        "\n",
        "        df_test = pd.DataFrame(X_test)\n",
        "        df_test['target'] = y_test\n",
        "        df_test.to_csv(f\"/content/data/test/test_dataset_{self.dataset_id}.csv\", index=False)\n",
        "\n",
        "        print(f\"Processed datasets saved: processed_dataset_{self.dataset_id}.csv, train_dataset_{self.dataset_id}.csv, test_dataset_{self.dataset_id}.csv\")\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        self.fetch_data()\n",
        "        X_processed, y = self.preprocess_data()\n",
        "        X_train, X_test, y_train, y_test = self.split_data(X_processed, y)\n",
        "        self.save_to_csv(X_processed, y, X_train, X_test, y_train, y_test)\n",
        "        print(\"Data pipeline completed.\")\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = OpenMLPipeline(dataset_id=46517)  # Example dataset ID\n",
        "    X_train, X_test, y_train, y_test = pipeline.run_pipeline()\n",
        "    print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "1A35aZyryBfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f036ee8a-f472-456a-9046-c8146175da0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching dataset 46517 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "fetch data: (5960, 20)\n",
            "Processed datasets saved: processed_dataset_46517.csv, train_dataset_46517.csv, test_dataset_46517.csv\n",
            "Data pipeline completed.\n",
            "Train shape: (4768, 20), Test shape: (1192, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openml\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class OpenMLPipeline:\n",
        "    def __init__(self, dataset_id):\n",
        "        self.dataset_id = dataset_id\n",
        "        self.dataset = None\n",
        "        self.features = None\n",
        "        self.labels = None\n",
        "        self.preprocessor = None\n",
        "\n",
        "    def fetch_data(self):\n",
        "        print(f\"Fetching dataset {self.dataset_id} from OpenML...\")\n",
        "        dataset = openml.datasets.get_dataset(self.dataset_id)\n",
        "        X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "        if self.labels.dtype in ['object', 'bool'] or self.labels.dtype.name == 'category':\n",
        "            print(\"Target is categorical, encoding to numerical...\")\n",
        "            self.labels = LabelEncoder().fit_transform(self.labels)\n",
        "\n",
        "        print(\"Data shape:\", X.shape)\n",
        "        return self.features, self.labels\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        if self.features is None or self.labels is None:\n",
        "            raise ValueError(\"Dataset not loaded. Call fetch_data() first.\")\n",
        "\n",
        "        numeric_features = self.features.select_dtypes(include=['int64', 'float64', 'uint8']).columns\n",
        "        categorical_features = self.features.select_dtypes(include=['object', 'category', 'bool']).columns\n",
        "\n",
        "        # Warn about columns being skipped\n",
        "        other_columns = self.features.columns.difference(numeric_features.union(categorical_features))\n",
        "        if len(other_columns) > 0:\n",
        "            print(f\"Warning: Skipping unrecognized feature types: {list(other_columns)}\")\n",
        "\n",
        "        numeric_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='mean')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ])\n",
        "\n",
        "        categorical_transformer = Pipeline(steps=[\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "\n",
        "        self.preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        X_processed = self.preprocessor.fit_transform(self.features)\n",
        "\n",
        "        X_processed = pd.DataFrame(X_processed, columns=self.preprocessor.get_feature_names_out())\n",
        "        return X_processed, self.labels\n",
        "\n",
        "    def save_to_csv(self, X, y):\n",
        "        os.makedirs(\"/content/data/processed\", exist_ok=True)\n",
        "\n",
        "        df_full = pd.DataFrame(X)\n",
        "        df_full['target'] = y\n",
        "        df_full.to_csv(f\"/content/data/processed/processed_dataset_{self.dataset_id}.csv\", index=False)\n",
        "\n",
        "        print(f\"Processed dataset saved to processed_dataset_{self.dataset_id}.csv\")\n",
        "\n",
        "    def run_pipeline(self):\n",
        "      self.fetch_data()\n",
        "      X_processed, y = self.preprocess_data()\n",
        "\n",
        "      if X_processed.shape[1] == 20:\n",
        "          self.save_to_csv(X_processed, y)\n",
        "          print(f\"Saved dataset {self.dataset_id} with shape {X_processed.shape}\")\n",
        "          return X_processed, y\n",
        "      else:\n",
        "          print(f\"Skipped dataset {self.dataset_id}: final dimension = {X_processed.shape[1]} (expected 20)\")\n",
        "          return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = OpenMLPipeline(dataset_id=1042)  # Example dataset ID\n",
        "    X_processed, y = pipeline.run_pipeline()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbacthWFSB-0",
        "outputId": "78fadb0c-3fbc-45a3-9042-9c45adbbd4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching dataset 1042 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (3468, 784)\n",
            "❌ Skipped dataset 1042: final dimension = 784 (expected 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0KwQwxHnocf",
        "outputId": "a32da11c-0085-43e6-e295-f42b61b0678d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running pipeline on dataset ID: 30\n",
            "Fetching dataset 30 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5473, 10)\n",
            "❌ Skipped dataset 30: final dimension = 10 (expected 20)\n",
            "Failed on dataset 30: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 189\n",
            "Fetching dataset 189 from OpenML...\n",
            "Data shape: (8192, 8)\n",
            "❌ Skipped dataset 189: final dimension = 8 (expected 20)\n",
            "Failed on dataset 189: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 225\n",
            "Fetching dataset 225 from OpenML...\n",
            "Data shape: (8192, 8)\n",
            "❌ Skipped dataset 225: final dimension = 8 (expected 20)\n",
            "Failed on dataset 225: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 227\n",
            "Fetching dataset 227 from OpenML...\n",
            "Data shape: (8192, 12)\n",
            "❌ Skipped dataset 227: final dimension = 12 (expected 20)\n",
            "Failed on dataset 227: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 287\n",
            "Fetching dataset 287 from OpenML...\n",
            "Data shape: (6497, 11)\n",
            "❌ Skipped dataset 287: final dimension = 11 (expected 20)\n",
            "Failed on dataset 287: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 375\n",
            "Fetching dataset 375 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (9961, 14)\n",
            "❌ Skipped dataset 375: final dimension = 14 (expected 20)\n",
            "Failed on dataset 375: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 503\n",
            "Fetching dataset 503 from OpenML...\n",
            "Data shape: (6574, 14)\n",
            "❌ Skipped dataset 503: final dimension = 14 (expected 20)\n",
            "Failed on dataset 503: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 562\n",
            "Fetching dataset 562 from OpenML...\n",
            "Data shape: (8192, 12)\n",
            "❌ Skipped dataset 562: final dimension = 12 (expected 20)\n",
            "Failed on dataset 562: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 572\n",
            "Fetching dataset 572 from OpenML...\n",
            "Data shape: (8192, 8)\n",
            "❌ Skipped dataset 572: final dimension = 8 (expected 20)\n",
            "Failed on dataset 572: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 725\n",
            "Fetching dataset 725 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (8192, 8)\n",
            "❌ Skipped dataset 725: final dimension = 8 (expected 20)\n",
            "Failed on dataset 725: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 735\n",
            "Fetching dataset 735 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (8192, 12)\n",
            "❌ Skipped dataset 735: final dimension = 12 (expected 20)\n",
            "Failed on dataset 735: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 807\n",
            "Fetching dataset 807 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (8192, 8)\n",
            "❌ Skipped dataset 807: final dimension = 8 (expected 20)\n",
            "Failed on dataset 807: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 816\n",
            "Fetching dataset 816 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (8192, 8)\n",
            "❌ Skipped dataset 816: final dimension = 8 (expected 20)\n",
            "Failed on dataset 816: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 847\n",
            "Fetching dataset 847 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (6574, 14)\n",
            "❌ Skipped dataset 847: final dimension = 14 (expected 20)\n",
            "Failed on dataset 847: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 976\n",
            "Fetching dataset 976 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (9961, 14)\n",
            "❌ Skipped dataset 976: final dimension = 14 (expected 20)\n",
            "Failed on dataset 976: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 1021\n",
            "Fetching dataset 1021 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5473, 10)\n",
            "❌ Skipped dataset 1021: final dimension = 10 (expected 20)\n",
            "Failed on dataset 1021: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 1496\n",
            "Fetching dataset 1496 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (7400, 20)\n",
            "Processed dataset saved to processed_dataset_1496.csv\n",
            "✅ Saved dataset 1496 with shape (7400, 20)\n",
            "Shape: (7400, 20), Target length: 7400\n",
            "\n",
            "Running pipeline on dataset ID: 1507\n",
            "Fetching dataset 1507 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (7400, 20)\n",
            "Processed dataset saved to processed_dataset_1507.csv\n",
            "✅ Saved dataset 1507 with shape (7400, 20)\n",
            "Shape: (7400, 20), Target length: 7400\n",
            "\n",
            "Running pipeline on dataset ID: 4552\n",
            "Fetching dataset 4552 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5665, 16)\n",
            "Failed on dataset 4552: Shape of passed values is (5665, 1), indices imply (5665, 104)\n",
            "\n",
            "Running pipeline on dataset ID: 41146\n",
            "Fetching dataset 41146 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5124, 20)\n",
            "Processed dataset saved to processed_dataset_41146.csv\n",
            "✅ Saved dataset 41146 with shape (5124, 20)\n",
            "Shape: (5124, 20), Target length: 5124\n",
            "\n",
            "Running pipeline on dataset ID: 42125\n",
            "Fetching dataset 42125 from OpenML...\n",
            "Data shape: (9228, 12)\n",
            "Failed on dataset 42125: Shape of passed values is (9228, 1), indices imply (9228, 12731)\n",
            "\n",
            "Running pipeline on dataset ID: 42178\n",
            "Fetching dataset 42178 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (7043, 19)\n",
            "Failed on dataset 42178: Shape of passed values is (7043, 1), indices imply (7043, 6575)\n",
            "\n",
            "Running pipeline on dataset ID: 42192\n",
            "Fetching dataset 42192 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5278, 13)\n",
            "❌ Skipped dataset 42192: final dimension = 19 (expected 20)\n",
            "Failed on dataset 42192: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 42193\n",
            "Fetching dataset 42193 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5278, 13)\n",
            "❌ Skipped dataset 42193: final dimension = 19 (expected 20)\n",
            "Failed on dataset 42193: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 42889\n",
            "Fetching dataset 42889 from OpenML...\n",
            "Data shape: (6321, 12)\n",
            "Failed on dataset 42889: Shape of passed values is (6321, 1), indices imply (6321, 1065)\n",
            "\n",
            "Running pipeline on dataset ID: 43431\n",
            "Fetching dataset 43431 from OpenML...\n",
            "Data shape: (6855, 9)\n",
            "Failed on dataset 43431: Shape of passed values is (6855, 1), indices imply (6855, 12962)\n",
            "\n",
            "Running pipeline on dataset ID: 43958\n",
            "Fetching dataset 43958 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5188, 20)\n",
            "Failed on dataset 43958: Shape of passed values is (5188, 1), indices imply (5188, 581)\n",
            "\n",
            "Running pipeline on dataset ID: 43970\n",
            "Fetching dataset 43970 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5188, 20)\n",
            "Failed on dataset 43970: Shape of passed values is (5188, 1), indices imply (5188, 581)\n",
            "\n",
            "Running pipeline on dataset ID: 43976\n",
            "Fetching dataset 43976 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (7608, 20)\n",
            "Failed on dataset 43976: Shape of passed values is (7608, 1), indices imply (7608, 183)\n",
            "\n",
            "Running pipeline on dataset ID: 43986\n",
            "Fetching dataset 43986 from OpenML...\n",
            "Data shape: (6497, 11)\n",
            "❌ Skipped dataset 43986: final dimension = 11 (expected 20)\n",
            "Failed on dataset 43986: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 43994\n",
            "Fetching dataset 43994 from OpenML...\n",
            "Data shape: (6497, 11)\n",
            "❌ Skipped dataset 43994: final dimension = 11 (expected 20)\n",
            "Failed on dataset 43994: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 44011\n",
            "Fetching dataset 44011 from OpenML...\n",
            "Data shape: (6497, 11)\n",
            "❌ Skipped dataset 44011: final dimension = 11 (expected 20)\n",
            "Failed on dataset 44011: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 44078\n",
            "Fetching dataset 44078 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (7608, 20)\n",
            "Failed on dataset 44078: Shape of passed values is (7608, 1), indices imply (7608, 183)\n",
            "\n",
            "Running pipeline on dataset ID: 44084\n",
            "Fetching dataset 44084 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5188, 20)\n",
            "Failed on dataset 44084: Shape of passed values is (5188, 1), indices imply (5188, 581)\n",
            "\n",
            "Running pipeline on dataset ID: 44093\n",
            "Fetching dataset 44093 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (7608, 20)\n",
            "Failed on dataset 44093: Shape of passed values is (7608, 1), indices imply (7608, 183)\n",
            "\n",
            "Running pipeline on dataset ID: 44115\n",
            "Fetching dataset 44115 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5188, 20)\n",
            "Processed dataset saved to processed_dataset_44115.csv\n",
            "✅ Saved dataset 44115 with shape (5188, 20)\n",
            "Shape: (5188, 20), Target length: 5188\n",
            "\n",
            "Running pipeline on dataset ID: 44124\n",
            "Fetching dataset 44124 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5188, 20)\n",
            "Processed dataset saved to processed_dataset_44124.csv\n",
            "✅ Saved dataset 44124 with shape (5188, 20)\n",
            "Shape: (5188, 20), Target length: 5188\n",
            "\n",
            "Running pipeline on dataset ID: 44130\n",
            "Fetching dataset 44130 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (7608, 20)\n",
            "Processed dataset saved to processed_dataset_44130.csv\n",
            "✅ Saved dataset 44130 with shape (7608, 20)\n",
            "Shape: (7608, 20), Target length: 7608\n",
            "\n",
            "Running pipeline on dataset ID: 44136\n",
            "Fetching dataset 44136 from OpenML...\n",
            "Data shape: (6497, 11)\n",
            "❌ Skipped dataset 44136: final dimension = 11 (expected 20)\n",
            "Failed on dataset 44136: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 44150\n",
            "Fetching dataset 44150 from OpenML...\n",
            "Data shape: (5692, 16)\n",
            "❌ Skipped dataset 44150: final dimension = 16 (expected 20)\n",
            "Failed on dataset 44150: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 44201\n",
            "Fetching dataset 44201 from OpenML...\n",
            "Data shape: (10000, 18)\n",
            "❌ Skipped dataset 44201: final dimension = 24 (expected 20)\n",
            "Failed on dataset 44201: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 44202\n",
            "Fetching dataset 44202 from OpenML...\n",
            "Data shape: (10000, 18)\n",
            "❌ Skipped dataset 44202: final dimension = 24 (expected 20)\n",
            "Failed on dataset 44202: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 44973\n",
            "Fetching dataset 44973 from OpenML...\n",
            "Data shape: (10000, 12)\n",
            "❌ Skipped dataset 44973: final dimension = 12 (expected 20)\n",
            "Failed on dataset 44973: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 44980\n",
            "Fetching dataset 44980 from OpenML...\n",
            "Data shape: (8192, 8)\n",
            "❌ Skipped dataset 44980: final dimension = 8 (expected 20)\n",
            "Failed on dataset 44980: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 45062\n",
            "Fetching dataset 45062 from OpenML...\n",
            "Data shape: (10000, 10)\n",
            "❌ Skipped dataset 45062: final dimension = 28 (expected 20)\n",
            "Failed on dataset 45062: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 45568\n",
            "Fetching dataset 45568 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (7043, 19)\n",
            "❌ Skipped dataset 45568: final dimension = 46 (expected 20)\n",
            "Failed on dataset 45568: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 45569\n",
            "Fetching dataset 45569 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (10000, 9)\n",
            "Failed on dataset 45569: Shape of passed values is (10000, 1), indices imply (10000, 37105)\n",
            "\n",
            "Running pipeline on dataset ID: 45586\n",
            "Fetching dataset 45586 from OpenML...\n",
            "Data shape: (10000, 13)\n",
            "Failed on dataset 45586: Shape of passed values is (10000, 1), indices imply (10000, 2947)\n",
            "\n",
            "Running pipeline on dataset ID: 46280\n",
            "Fetching dataset 46280 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (7043, 19)\n",
            "❌ Skipped dataset 46280: final dimension = 46 (expected 20)\n",
            "Failed on dataset 46280: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 46297\n",
            "Fetching dataset 46297 from OpenML...\n",
            "Data shape: (8760, 13)\n",
            "Failed on dataset 46297: Shape of passed values is (8760, 1), indices imply (8760, 382)\n",
            "\n",
            "Running pipeline on dataset ID: 46298\n",
            "Fetching dataset 46298 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (10000, 12)\n",
            "❌ Skipped dataset 46298: final dimension = 12 (expected 20)\n",
            "Failed on dataset 46298: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 46299\n",
            "Fetching dataset 46299 from OpenML...\n",
            "Data shape: (10000, 12)\n",
            "❌ Skipped dataset 46299: final dimension = 12 (expected 20)\n",
            "Failed on dataset 46299: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 46328\n",
            "Fetching dataset 46328 from OpenML...\n",
            "Data shape: (8760, 17)\n",
            "❌ Skipped dataset 46328: final dimension = 28 (expected 20)\n",
            "Failed on dataset 46328: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 46431\n",
            "Fetching dataset 46431 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5960, 20)\n",
            "❌ Skipped dataset 46431: final dimension = 30 (expected 20)\n",
            "Failed on dataset 46431: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 46483\n",
            "Fetching dataset 46483 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5960, 12)\n",
            "Processed dataset saved to processed_dataset_46483.csv\n",
            "✅ Saved dataset 46483 with shape (5960, 20)\n",
            "Shape: (5960, 20), Target length: 5960\n",
            "\n",
            "Running pipeline on dataset ID: 46504\n",
            "Fetching dataset 46504 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5227, 20)\n",
            "Processed dataset saved to processed_dataset_46504.csv\n",
            "✅ Saved dataset 46504 with shape (5227, 20)\n",
            "Shape: (5227, 20), Target length: 5227\n",
            "\n",
            "Running pipeline on dataset ID: 46517\n",
            "Fetching dataset 46517 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (5960, 20)\n",
            "Processed dataset saved to processed_dataset_46517.csv\n",
            "✅ Saved dataset 46517 with shape (5960, 20)\n",
            "Shape: (5960, 20), Target length: 5960\n",
            "\n",
            "Running pipeline on dataset ID: 46726\n",
            "Fetching dataset 46726 from OpenML...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openml.datasets.functions:Could not download file from https://data.openml.org/datasets/0004/46726/dataset_46726.pq: Object at 'https://data.openml.org/datasets/0004/46726/dataset_46726.pq' does not exist.\n",
            "WARNING:root:Received uncompressed content from OpenML for https://api.openml.org/data/v1/download/22121426/climate_change_impact_on_agriculture_2024.arff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (10000, 14)\n",
            "Failed on dataset 46726: Shape of passed values is (10000, 1), indices imply (10000, 69)\n",
            "\n",
            "Running pipeline on dataset ID: 46729\n",
            "Fetching dataset 46729 from OpenML...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openml.datasets.functions:Could not download file from https://data.openml.org/datasets/0004/46729/dataset_46729.pq: Object at 'https://data.openml.org/datasets/0004/46729/dataset_46729.pq' does not exist.\n",
            "WARNING:root:Received uncompressed content from OpenML for https://api.openml.org/data/v1/download/22121429/climate_insights_dataset.arff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (10000, 8)\n",
            "Failed on dataset 46729: Shape of passed values is (10000, 1), indices imply (10000, 18012)\n",
            "\n",
            "Running pipeline on dataset ID: 46748\n",
            "Fetching dataset 46748 from OpenML...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:openml.datasets.functions:Could not download file from https://data.openml.org/datasets/0004/46748/dataset_46748.pq: Object at 'https://data.openml.org/datasets/0004/46748/dataset_46748.pq' does not exist.\n",
            "WARNING:root:Received uncompressed content from OpenML for https://api.openml.org/data/v1/download/22121448/coffee_distribution_across_94_counties.arff.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (6016, 20)\n",
            "Failed on dataset 46748: Shape of passed values is (6016, 1), indices imply (6016, 113)\n",
            "\n",
            "Running pipeline on dataset ID: 46911\n",
            "Fetching dataset 46911 from OpenML...\n",
            "Target is categorical, encoding to numerical...\n",
            "Data shape: (10000, 10)\n",
            "❌ Skipped dataset 46911: final dimension = 15 (expected 20)\n",
            "Failed on dataset 46911: 'NoneType' object has no attribute 'shape'\n",
            "\n",
            "Running pipeline on dataset ID: 46964\n",
            "Fetching dataset 46964 from OpenML...\n",
            "Data shape: (6497, 12)\n",
            "❌ Skipped dataset 46964: final dimension = 13 (expected 20)\n",
            "Failed on dataset 46964: 'NoneType' object has no attribute 'shape'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for id in id_list:\n",
        "    try:\n",
        "        print(f\"Running pipeline on dataset ID: {id}\")\n",
        "        pipeline = OpenMLPipeline(dataset_id=id)\n",
        "        X_processed, y = pipeline.run_pipeline()\n",
        "        print(f\"Shape: {X_processed.shape}, Target length: {len(y)}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed on dataset {id}: {e}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive('/content/data_folder', 'zip', '/content/data')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sjXznEhEe8Wx",
        "outputId": "bc13de8a-1159-46c2-d5f7-c07242e6b85a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/data_folder.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/data_folder.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MuLaPY8ofAHg",
        "outputId": "87f93a9a-645c-4646-86d8-a4e8acb8c3fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_79a9ea6d-7ba5-4fc7-bd76-be6e434b63a0\", \"data_folder.zip\", 364511)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}