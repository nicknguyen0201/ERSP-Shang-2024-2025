{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example usage of inferencing with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# limit gpu usage to one gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Alternatively, use LLAMA 3.2 1B Instruct\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "movie_review = \"The movie was great, I loved it!\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Tokenize the input text\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_inputs = tokenizer([f\"Analyze the sentiment of the movie review: <{movie_review}>, output positive or negative.\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate text using the model\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load IMDB dataset\n",
    "dataset = load_dataset(\"yzhuang/imdb\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"validation\"]\n",
    "\n",
    "test_accuracy = 0\n",
    "# iterate over the test dataset\n",
    "for data in test_dataset:\n",
    "    movie_review = data[\"text\"]\n",
    "    ground_truth = data[\"label\"] # \"negative. or positive.\"\n",
    "\n",
    "    # TODO: do some thing over here\n",
    "\n",
    "    # ENDTODO\n",
    "\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function of Transformers, starting point, Nov 27\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained()\n",
    "# input = [n, d] X\n",
    "# output = [k, d] Y\n",
    "\n",
    "Y = torch.randn(k, d)\n",
    "for number_of_optimization in range(100):\n",
    "    # forward pass\n",
    "    X = input_x\n",
    "    model_input = torch.cat([X, Y], dim=0) #[X, Y]\n",
    "    output = model(model_input) # in [n+k, d]\n",
    "    new_Y = output[-k:] # [k, d]\n",
    "\n",
    "    loss = inertia_loss(X, new_Y)\n",
    "    Y = new_Y\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # optimization\n",
    "    optimizer.step()\n",
    "\n",
    "    # print loss\n",
    "    print(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
